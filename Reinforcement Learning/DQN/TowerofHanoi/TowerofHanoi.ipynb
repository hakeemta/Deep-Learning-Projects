{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from ray.rllib.algorithms.dqn.dqn import DQNConfig\n",
    "from ray.rllib.algorithms.ppo.ppo import PPOConfig\n",
    "from ray.rllib.algorithms.a3c.a3c import A3CConfig\n",
    "from ray.rllib.algorithms.a2c.a2c import A2CConfig\n",
    "from ray.rllib.algorithms.appo.appo import APPOConfig\n",
    "from ray.rllib.algorithms.impala.impala import ImpalaConfig\n",
    "from ray.rllib.algorithms.pg.pg import PGConfig\n",
    "from ray.rllib.algorithms.sac.sac import SACConfig\n",
    "\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from towerofhanoienv import TowerOfHanoiEnv, EmbeddingEnv, FlattenEnv\n",
    "from preprocessing import AutoEncoder, Dataset\n",
    "\n",
    "\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_root = \"tmp/dqn/towerofhanoi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=1264)\u001b[0m /Users/sophialawal/opt/anaconda3/envs/tf/lib/python3.10/site-packages/gymnasium/spaces/multi_discrete.py:177: UserWarning: \u001b[33mWARN: Getting the length of a multi-dimensional MultiDiscrete space.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1264)\u001b[0m   gym.logger.warn(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1263)\u001b[0m /Users/sophialawal/opt/anaconda3/envs/tf/lib/python3.10/site-packages/gymnasium/spaces/multi_discrete.py:177: UserWarning: \u001b[33mWARN: Getting the length of a multi-dimensional MultiDiscrete space.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1263)\u001b[0m   gym.logger.warn(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1265)\u001b[0m /Users/sophialawal/opt/anaconda3/envs/tf/lib/python3.10/site-packages/gymnasium/spaces/multi_discrete.py:177: UserWarning: \u001b[33mWARN: Getting the length of a multi-dimensional MultiDiscrete space.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1265)\u001b[0m   gym.logger.warn(\n",
      "2023-03-11 00:38:37,369\tINFO trainable.py:172 -- Trainable.setup took 14.660 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2023-03-11 00:38:37,372\tWARNING util.py:67 -- Install gputil for GPU system monitoring.\n",
      "2023-03-11 00:38:37,724\tWARNING multi_agent_prioritized_replay_buffer.py:215 -- Adding batches with column `weights` to this buffer while providing weights as a call argument to the add method results in the column being overwritten.\n",
      "2023-03-11 00:38:39,451\tWARNING policy.py:126 -- Can not figure out a durable policy name for <class 'ray.rllib.policy.eager_tf_policy.SACTFPolicy_eager'>. You are probably trying to checkpoint a custom policy. Raw policy class may cause problems when the checkpoint needs to be loaded in the future. To fix this, make sure you add your custom policy in rllib.algorithms.registry.POLICIES.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0: Min/Mean/Max reward/episode length mean:      nan/     nan/     nan/     nan. Checkpoint saved to tmp/dqn/towerofhanoi/checkpoint_000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-11 00:38:40,651\tWARNING policy.py:126 -- Can not figure out a durable policy name for <class 'ray.rllib.policy.eager_tf_policy.SACTFPolicy_eager'>. You are probably trying to checkpoint a custom policy. Raw policy class may cause problems when the checkpoint needs to be loaded in the future. To fix this, make sure you add your custom policy in rllib.algorithms.registry.POLICIES.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1: Min/Mean/Max reward/episode length mean:      nan/     nan/     nan/     nan. Checkpoint saved to tmp/dqn/towerofhanoi/checkpoint_000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-11 00:38:41,810\tWARNING policy.py:126 -- Can not figure out a durable policy name for <class 'ray.rllib.policy.eager_tf_policy.SACTFPolicy_eager'>. You are probably trying to checkpoint a custom policy. Raw policy class may cause problems when the checkpoint needs to be loaded in the future. To fix this, make sure you add your custom policy in rllib.algorithms.registry.POLICIES.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2: Min/Mean/Max reward/episode length mean:      nan/     nan/     nan/     nan. Checkpoint saved to tmp/dqn/towerofhanoi/checkpoint_000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-11 00:38:42,916\tWARNING policy.py:126 -- Can not figure out a durable policy name for <class 'ray.rllib.policy.eager_tf_policy.SACTFPolicy_eager'>. You are probably trying to checkpoint a custom policy. Raw policy class may cause problems when the checkpoint needs to be loaded in the future. To fix this, make sure you add your custom policy in rllib.algorithms.registry.POLICIES.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3: Min/Mean/Max reward/episode length mean:      nan/     nan/     nan/     nan. Checkpoint saved to tmp/dqn/towerofhanoi/checkpoint_000005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-11 00:38:44,023\tWARNING policy.py:126 -- Can not figure out a durable policy name for <class 'ray.rllib.policy.eager_tf_policy.SACTFPolicy_eager'>. You are probably trying to checkpoint a custom policy. Raw policy class may cause problems when the checkpoint needs to be loaded in the future. To fix this, make sure you add your custom policy in rllib.algorithms.registry.POLICIES.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4: Min/Mean/Max reward/episode length mean:      nan/     nan/     nan/     nan. Checkpoint saved to tmp/dqn/towerofhanoi/checkpoint_000006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-11 00:38:52,052\tWARNING policy.py:126 -- Can not figure out a durable policy name for <class 'ray.rllib.policy.eager_tf_policy.SACTFPolicy_eager'>. You are probably trying to checkpoint a custom policy. Raw policy class may cause problems when the checkpoint needs to be loaded in the future. To fix this, make sure you add your custom policy in rllib.algorithms.registry.POLICIES.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5: Min/Mean/Max reward/episode length mean:      nan/     nan/     nan/     nan. Checkpoint saved to tmp/dqn/towerofhanoi/checkpoint_000007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-11 00:39:02,722\tWARNING policy.py:126 -- Can not figure out a durable policy name for <class 'ray.rllib.policy.eager_tf_policy.SACTFPolicy_eager'>. You are probably trying to checkpoint a custom policy. Raw policy class may cause problems when the checkpoint needs to be loaded in the future. To fix this, make sure you add your custom policy in rllib.algorithms.registry.POLICIES.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6: Min/Mean/Max reward/episode length mean:      nan/     nan/     nan/     nan. Checkpoint saved to tmp/dqn/towerofhanoi/checkpoint_000008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-11 00:39:14,260\tWARNING policy.py:126 -- Can not figure out a durable policy name for <class 'ray.rllib.policy.eager_tf_policy.SACTFPolicy_eager'>. You are probably trying to checkpoint a custom policy. Raw policy class may cause problems when the checkpoint needs to be loaded in the future. To fix this, make sure you add your custom policy in rllib.algorithms.registry.POLICIES.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7: Min/Mean/Max reward/episode length mean:      nan/     nan/     nan/     nan. Checkpoint saved to tmp/dqn/towerofhanoi/checkpoint_000009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-11 00:39:24,340\tWARNING policy.py:126 -- Can not figure out a durable policy name for <class 'ray.rllib.policy.eager_tf_policy.SACTFPolicy_eager'>. You are probably trying to checkpoint a custom policy. Raw policy class may cause problems when the checkpoint needs to be loaded in the future. To fix this, make sure you add your custom policy in rllib.algorithms.registry.POLICIES.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8: Min/Mean/Max reward/episode length mean:      nan/     nan/     nan/     nan. Checkpoint saved to tmp/dqn/towerofhanoi/checkpoint_000010\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/sophialawal/Visual_Studio/Deep_learning_projects/Reinforcement Learning/DQN/TowerofHanoi/TowerofHanoi.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sophialawal/Visual_Studio/Deep_learning_projects/Reinforcement%20Learning/DQN/TowerofHanoi/TowerofHanoi.ipynb#W2sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m episode_json \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sophialawal/Visual_Studio/Deep_learning_projects/Reinforcement%20Learning/DQN/TowerofHanoi/TowerofHanoi.ipynb#W2sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m15\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sophialawal/Visual_Studio/Deep_learning_projects/Reinforcement%20Learning/DQN/TowerofHanoi/TowerofHanoi.ipynb#W2sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     result \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sophialawal/Visual_Studio/Deep_learning_projects/Reinforcement%20Learning/DQN/TowerofHanoi/TowerofHanoi.ipynb#W2sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     results\u001b[39m.\u001b[39mappend(result)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sophialawal/Visual_Studio/Deep_learning_projects/Reinforcement%20Learning/DQN/TowerofHanoi/TowerofHanoi.ipynb#W2sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     episode \u001b[39m=\u001b[39m {\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sophialawal/Visual_Studio/Deep_learning_projects/Reinforcement%20Learning/DQN/TowerofHanoi/TowerofHanoi.ipynb#W2sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mn\u001b[39m\u001b[39m\"\u001b[39m: n,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sophialawal/Visual_Studio/Deep_learning_projects/Reinforcement%20Learning/DQN/TowerofHanoi/TowerofHanoi.ipynb#W2sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mepisode_reward_min\u001b[39m\u001b[39m\"\u001b[39m: result[\u001b[39m\"\u001b[39m\u001b[39mepisode_reward_min\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sophialawal/Visual_Studio/Deep_learning_projects/Reinforcement%20Learning/DQN/TowerofHanoi/TowerofHanoi.ipynb#W2sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mepisode_len_mean\u001b[39m\u001b[39m\"\u001b[39m: result[\u001b[39m\"\u001b[39m\u001b[39mepisode_len_mean\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sophialawal/Visual_Studio/Deep_learning_projects/Reinforcement%20Learning/DQN/TowerofHanoi/TowerofHanoi.ipynb#W2sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     }\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ray/tune/trainable/trainable.py:365\u001b[0m, in \u001b[0;36mTrainable.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    363\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    364\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 365\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m    366\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    367\u001b[0m     skipped \u001b[39m=\u001b[39m skip_exceptions(e)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:782\u001b[0m, in \u001b[0;36mAlgorithm.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    774\u001b[0m     (\n\u001b[1;32m    775\u001b[0m         results,\n\u001b[1;32m    776\u001b[0m         train_iter_ctx,\n\u001b[1;32m    777\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_one_training_iteration_and_evaluation_in_parallel()\n\u001b[1;32m    778\u001b[0m \u001b[39m# - No evaluation necessary, just run the next training iteration.\u001b[39;00m\n\u001b[1;32m    779\u001b[0m \u001b[39m# - We have to evaluate in this training iteration, but no parallelism ->\u001b[39;00m\n\u001b[1;32m    780\u001b[0m \u001b[39m#   evaluate after the training iteration is entirely done.\u001b[39;00m\n\u001b[1;32m    781\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 782\u001b[0m     results, train_iter_ctx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_one_training_iteration()\n\u001b[1;32m    784\u001b[0m \u001b[39m# Sequential: Train (already done above), then evaluate.\u001b[39;00m\n\u001b[1;32m    785\u001b[0m \u001b[39mif\u001b[39;00m evaluate_this_iter \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mevaluation_parallel_to_training:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:2713\u001b[0m, in \u001b[0;36mAlgorithm._run_one_training_iteration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2711\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timers[TRAINING_ITERATION_TIMER]:\n\u001b[1;32m   2712\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39m_disable_execution_plan_api:\n\u001b[0;32m-> 2713\u001b[0m         results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step()\n\u001b[1;32m   2714\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2715\u001b[0m         results \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_exec_impl)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ray/rllib/algorithms/dqn/dqn.py:430\u001b[0m, in \u001b[0;36mDQN.training_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[39mif\u001b[39;00m cur_ts \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_steps_sampled_before_learning_starts:\n\u001b[1;32m    428\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(sample_and_train_weight):\n\u001b[1;32m    429\u001b[0m         \u001b[39m# Sample training batch (MultiAgentBatch) from replay buffer.\u001b[39;00m\n\u001b[0;32m--> 430\u001b[0m         train_batch \u001b[39m=\u001b[39m sample_min_n_steps_from_buffer(\n\u001b[1;32m    431\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlocal_replay_buffer,\n\u001b[1;32m    432\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mtrain_batch_size,\n\u001b[1;32m    433\u001b[0m             count_by_agent_steps\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mcount_steps_by \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39magent_steps\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    434\u001b[0m         )\n\u001b[1;32m    436\u001b[0m         \u001b[39m# Postprocess batch before we learn on it\u001b[39;00m\n\u001b[1;32m    437\u001b[0m         post_fn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mbefore_learn_on_batch\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m (\u001b[39mlambda\u001b[39;00m b, \u001b[39m*\u001b[39ma: b)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ray/rllib/utils/replay_buffers/utils.py:147\u001b[0m, in \u001b[0;36msample_min_n_steps_from_buffer\u001b[0;34m(replay_buffer, min_steps, count_by_agent_steps)\u001b[0m\n\u001b[1;32m    145\u001b[0m train_batches \u001b[39m=\u001b[39m []\n\u001b[1;32m    146\u001b[0m \u001b[39mwhile\u001b[39;00m train_batch_size \u001b[39m<\u001b[39m min_steps:\n\u001b[0;32m--> 147\u001b[0m     batch \u001b[39m=\u001b[39m replay_buffer\u001b[39m.\u001b[39;49msample(num_items\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m    148\u001b[0m     batch_len \u001b[39m=\u001b[39m batch\u001b[39m.\u001b[39magent_steps() \u001b[39mif\u001b[39;00m count_by_agent_steps \u001b[39melse\u001b[39;00m batch\u001b[39m.\u001b[39menv_steps()\n\u001b[1;32m    149\u001b[0m     \u001b[39mif\u001b[39;00m batch_len \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    150\u001b[0m         \u001b[39m# Replay has not started, so we can't accumulate timesteps here\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ray/rllib/utils/replay_buffers/multi_agent_replay_buffer.py:331\u001b[0m, in \u001b[0;36mMultiAgentReplayBuffer.sample\u001b[0;34m(self, num_items, policy_id, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m samples \u001b[39m=\u001b[39m {}\n\u001b[1;32m    330\u001b[0m \u001b[39mfor\u001b[39;00m policy_id, replay_buffer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplay_buffers\u001b[39m.\u001b[39mitems():\n\u001b[0;32m--> 331\u001b[0m     samples[policy_id] \u001b[39m=\u001b[39m replay_buffer\u001b[39m.\u001b[39;49msample(num_items, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    332\u001b[0m \u001b[39mreturn\u001b[39;00m MultiAgentBatch(samples, \u001b[39msum\u001b[39m(s\u001b[39m.\u001b[39mcount \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m samples\u001b[39m.\u001b[39mvalues()))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ray/rllib/utils/replay_buffers/prioritized_replay_buffer.py:154\u001b[0m, in \u001b[0;36mPrioritizedReplayBuffer.sample\u001b[0;34m(self, num_items, beta, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     batch_indexes\u001b[39m.\u001b[39mextend([idx] \u001b[39m*\u001b[39m actual_size)\n\u001b[1;32m    153\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_timesteps_sampled \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m count\n\u001b[0;32m--> 154\u001b[0m batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_encode_sample(idxes)\n\u001b[1;32m    156\u001b[0m \u001b[39m# Note: prioritization is not supported in multi agent lockstep\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(batch, SampleBatch):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ray/rllib/utils/replay_buffers/replay_buffer.py:382\u001b[0m, in \u001b[0;36mReplayBuffer._encode_sample\u001b[0;34m(self, idxes)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m     out \u001b[39m=\u001b[39m SampleBatch()\n\u001b[0;32m--> 382\u001b[0m out\u001b[39m.\u001b[39;49mdecompress_if_needed()\n\u001b[1;32m    383\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ray/rllib/policy/sample_batch.py:990\u001b[0m, in \u001b[0;36mSampleBatch.decompress_if_needed\u001b[0;34m(self, columns)\u001b[0m\n\u001b[1;32m    987\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(value) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m is_compressed(value[\u001b[39m0\u001b[39m]):\n\u001b[1;32m    988\u001b[0m         curr[path[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([unpack(o) \u001b[39mfor\u001b[39;00m o \u001b[39min\u001b[39;00m value])\n\u001b[0;32m--> 990\u001b[0m tree\u001b[39m.\u001b[39;49mmap_structure_with_path(_decompress_in_place, \u001b[39mself\u001b[39;49m)\n\u001b[1;32m    992\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/tree/__init__.py:474\u001b[0m, in \u001b[0;36mmap_structure_with_path\u001b[0;34m(func, *structures, **kwargs)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmap_structure_with_path\u001b[39m(func, \u001b[39m*\u001b[39mstructures, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    439\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Maps `func` through given structures.\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \n\u001b[1;32m    441\u001b[0m \u001b[39m  This is a variant of :func:`~tree.map_structure` which accumulates\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[39m      than `check_types` is provided.\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 474\u001b[0m   \u001b[39mreturn\u001b[39;00m map_structure_with_path_up_to(structures[\u001b[39m0\u001b[39;49m], func, \u001b[39m*\u001b[39;49mstructures,\n\u001b[1;32m    475\u001b[0m                                        \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/tree/__init__.py:779\u001b[0m, in \u001b[0;36mmap_structure_with_path_up_to\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[39mfor\u001b[39;00m path_and_values \u001b[39min\u001b[39;00m _multiyield_flat_up_to(shallow_structure, \u001b[39m*\u001b[39mstructures):\n\u001b[1;32m    778\u001b[0m   results\u001b[39m.\u001b[39mappend(func(\u001b[39m*\u001b[39mpath_and_values))\n\u001b[0;32m--> 779\u001b[0m \u001b[39mreturn\u001b[39;00m unflatten_as(shallow_structure, results)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/tree/__init__.py:381\u001b[0m, in \u001b[0;36munflatten_as\u001b[0;34m(structure, flat_sequence)\u001b[0m\n\u001b[1;32m    375\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    376\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mCould not pack sequence. Structure had \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m elements, but flat_sequence \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    377\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mhad \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m elements.  Structure: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, flat_sequence: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    378\u001b[0m       \u001b[39m%\u001b[39m (\u001b[39mlen\u001b[39m(flat_structure), \u001b[39mlen\u001b[39m(flat_sequence), structure, flat_sequence))\n\u001b[1;32m    380\u001b[0m _, packed \u001b[39m=\u001b[39m _packed_nest_with_indices(structure, flat_sequence, \u001b[39m0\u001b[39m)\n\u001b[0;32m--> 381\u001b[0m \u001b[39mreturn\u001b[39;00m _sequence_like(structure, packed)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/tree/sequence.py:84\u001b[0m, in \u001b[0;36m_sequence_like\u001b[0;34m(instance, args)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(instance)(\u001b[39mdict\u001b[39m(keys_and_values))\n\u001b[1;32m     83\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39;49m(instance)(keys_and_values)\n\u001b[1;32m     85\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(instance, collections_abc\u001b[39m.\u001b[39mMappingView):\n\u001b[1;32m     86\u001b[0m   \u001b[39m# We can't directly construct mapping views, so we create a list instead\u001b[39;00m\n\u001b[1;32m     87\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(args)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ray/rllib/policy/sample_batch.py:258\u001b[0m, in \u001b[0;36mSampleBatch.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(v, (Number, \u001b[39mlist\u001b[39m)):\n\u001b[1;32m    256\u001b[0m         \u001b[39mself\u001b[39m[k] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(v)\n\u001b[0;32m--> 258\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcount \u001b[39m=\u001b[39m attempt_count_timesteps(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    260\u001b[0m \u001b[39m# A convenience map for slicing this batch into sub-batches along\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \u001b[39m# the time axis. This helps reduce repeated iterations through the\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[39m# batch's seq_lens array to find good slicing points. Built lazily\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[39m# when needed.\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slice_map \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ray/rllib/policy/sample_batch.py:77\u001b[0m, in \u001b[0;36mattempt_count_timesteps\u001b[0;34m(tensor_dict)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[39m# If this is a nested dict (for example a nested observation),\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[39m# try to flatten it, assert that all elements have the same length (batch\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[39m# dimension)\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m v_list \u001b[39m=\u001b[39m tree\u001b[39m.\u001b[39mflatten(v) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39;49m(v, (\u001b[39mdict\u001b[39;49m, \u001b[39mtuple\u001b[39;49m)) \u001b[39melse\u001b[39;00m [v]\n\u001b[1;32m     78\u001b[0m \u001b[39m# TODO: Drop support for lists and Numbers as values.\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[39m# If v_list contains lists or Numbers, convert them to arrays, too.\u001b[39;00m\n\u001b[1;32m     80\u001b[0m v_list \u001b[39m=\u001b[39m [\n\u001b[1;32m     81\u001b[0m     np\u001b[39m.\u001b[39marray(_v) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(_v, (Number, \u001b[39mlist\u001b[39m)) \u001b[39melse\u001b[39;00m _v \u001b[39mfor\u001b[39;00m _v \u001b[39min\u001b[39;00m v_list\n\u001b[1;32m     82\u001b[0m ]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "config = SACConfig()\n",
    "config = config.rollouts(num_rollout_workers=3) \n",
    "config =config.framework(\"tf2\") \n",
    "config = config.environment(TowerOfHanoiEnv, env_config={\"num_disks\": 3}, disable_env_checking=True) \n",
    "config = config.training(model={\"_disable_preprocessor_api\": False,\"use_lstm\":False}, lr=0.01)\n",
    "\n",
    "agent = config.build()  \n",
    "agent.train()  \n",
    "\n",
    "results = []\n",
    "episode_data = []\n",
    "episode_json = []\n",
    "\n",
    "for n in range(15):\n",
    "    result = agent.train()\n",
    "    results.append(result)\n",
    "\n",
    "    episode = {\n",
    "        \"n\": n,\n",
    "        \"episode_reward_min\": result[\"episode_reward_min\"],\n",
    "        \"episode_reward_mean\": result[\"episode_reward_mean\"],\n",
    "        \"episode_reward_max\": result[\"episode_reward_max\"],\n",
    "        \"episode_len_mean\": result[\"episode_len_mean\"],\n",
    "    }\n",
    "\n",
    "    episode_data.append(episode)\n",
    "    episode_json.append(json.dumps(episode))\n",
    "    file_name = agent.save(checkpoint_root)\n",
    "\n",
    "    print(f'{n:3d}: Min/Mean/Max reward/episode length mean: {result[\"episode_reward_min\"]:8.4f}/{result[\"episode_reward_mean\"]:8.4f}/{result[\"episode_reward_max\"]:8.4f}/{result[\"episode_len_mean\"]:8.4f}. Checkpoint saved to {file_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=episode_data)\n",
    "df.plot(x=\"n\", y=[\"episode_reward_mean\", \"episode_reward_min\", \"episode_reward_max\"], secondary_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ppo.PPO(env=TowerOfHanoiEnv, config={\n",
    "    \"env_config\": {\"num_disks\": 2},\n",
    "    \"num_workers\": 2,\n",
    "    #\"train_batch_size\": 16,\n",
    "    \"lr\": 0.00005,\n",
    "    \"model\": {\n",
    "        \"fcnet_hiddens\": [128, 256],\n",
    "        \"fcnet_activation\": \"relu\",\n",
    "    },\n",
    "\n",
    "})\n",
    "\n",
    "results = []\n",
    "episode_data = []\n",
    "episode_json = []\n",
    "\n",
    "for n in range(30):\n",
    "    result = agent.train()\n",
    "    results.append(result)\n",
    "\n",
    "    episode = {\n",
    "        \"n\": n,\n",
    "        \"episode_reward_min\": result[\"episode_reward_min\"],\n",
    "        \"episode_reward_mean\": result[\"episode_reward_mean\"],\n",
    "        \"episode_reward_max\": result[\"episode_reward_max\"],\n",
    "        \"episode_len_mean\": result[\"episode_len_mean\"],\n",
    "    }\n",
    "\n",
    "    episode_data.append(episode)\n",
    "    episode_json.append(json.dumps(episode))\n",
    "    file_name = agent.save(checkpoint_root)\n",
    "\n",
    "    print(f'{n:3d}: Min/Mean/Max reward: {result[\"episode_reward_min\"]:8.4f}/{result[\"episode_reward_mean\"]:8.4f}/{result[\"episode_reward_max\"]:8.4f}. Checkpoint saved to {file_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=episode_data)\n",
    "df.plot(x=\"n\", y=[\"episode_reward_mean\", \"episode_reward_min\", \"episode_reward_max\"], secondary_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent =DQN(env=TowerOfHanoiEnv, config={\n",
    "    #\"model\": {\n",
    "    #    \"custom_preprocessor\": MyPreprocessor,\n",
    "        # other model configuration parameters\n",
    "    #}\n",
    "    # other algorithm configuration parameters\n",
    "    \"env_config\": {\"num_disks\": 1},\n",
    "    #\"num_workers\": 2,\n",
    "    \"train_batch_size\": 16,\n",
    "    \"lr\": 0.00005,\n",
    "\n",
    "})\n",
    "\n",
    "results = []\n",
    "episode_data = []\n",
    "episode_json = []\n",
    "\n",
    "for n in range(30):\n",
    "    result = agent.train()\n",
    "    results.append(result)\n",
    "\n",
    "    episode = {\n",
    "        \"n\": n,\n",
    "        \"episode_reward_min\": result[\"episode_reward_min\"],\n",
    "        \"episode_reward_mean\": result[\"episode_reward_mean\"],\n",
    "        \"episode_reward_max\": result[\"episode_reward_max\"],\n",
    "        \"episode_len_mean\": result[\"episode_len_mean\"],\n",
    "    }\n",
    "\n",
    "    episode_data.append(episode)\n",
    "    episode_json.append(json.dumps(episode))\n",
    "    file_name = agent.save(checkpoint_root)\n",
    "\n",
    "    print(f'{n:3d}: Min/Mean/Max reward: {result[\"episode_reward_min\"]:8.4f}/{result[\"episode_reward_mean\"]:8.4f}/{result[\"episode_reward_max\"]:8.4f}. Checkpoint saved to {file_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=episode_data)\n",
    "df.plot(x=\"n\", y=[\"episode_reward_mean\", \"episode_reward_min\", \"episode_reward_max\"], secondary_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent =DQN(env=TowerOfHanoiEnv, config={\n",
    "    \"env_config\": {\"num_disks\": 5},\n",
    "    #\"num_workers\": 2,\n",
    "    \"train_batch_size\": 16,\n",
    "    \"lr\": 0.00005,\n",
    "\n",
    "})\n",
    "\n",
    "results = []\n",
    "episode_data = []\n",
    "episode_json = []\n",
    "\n",
    "for n in range(30):\n",
    "    result = agent.train()\n",
    "    results.append(result)\n",
    "\n",
    "    episode = {\n",
    "        \"n\": n,\n",
    "        \"episode_reward_min\": result[\"episode_reward_min\"],\n",
    "        \"episode_reward_mean\": result[\"episode_reward_mean\"],\n",
    "        \"episode_reward_max\": result[\"episode_reward_max\"],\n",
    "        \"episode_len_mean\": result[\"episode_len_mean\"],\n",
    "    }\n",
    "\n",
    "    episode_data.append(episode)\n",
    "    episode_json.append(json.dumps(episode))\n",
    "    file_name = agent.save(checkpoint_root)\n",
    "\n",
    "    print(f'{n:3d}: Min/Mean/Max reward: {result[\"episode_reward_min\"]:8.4f}/{result[\"episode_reward_mean\"]:8.4f}/{result[\"episode_reward_max\"]:8.4f}. Checkpoint saved to {file_name}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Preprocessor - Flatten Observation Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register gym env in rllib\n",
    "\n",
    "def env_creator(env_config):\n",
    "    return FlattenEnv(TowerOfHanoiEnv(env_config))\n",
    "\n",
    "register_env(\"TowerOfHanoiEnv\", env_creator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent =DQN(env=TowerOfHanoiEnv, config={\n",
    "\n",
    "    \"env_config\": {\"num_disks\": 1},\n",
    "    #\"num_workers\": 2,\n",
    "    \"train_batch_size\": 16,\n",
    "    \"lr\": 0.00005,\n",
    "\n",
    "})\n",
    "\n",
    "results = []\n",
    "episode_data = []\n",
    "episode_json = []\n",
    "\n",
    "for n in range(30):\n",
    "    result = agent.train()\n",
    "    results.append(result)\n",
    "\n",
    "    episode = {\n",
    "        \"n\": n,\n",
    "        \"episode_reward_min\": result[\"episode_reward_min\"],\n",
    "        \"episode_reward_mean\": result[\"episode_reward_mean\"],\n",
    "        \"episode_reward_max\": result[\"episode_reward_max\"],\n",
    "        \"episode_len_mean\": result[\"episode_len_mean\"],\n",
    "    }\n",
    "\n",
    "    episode_data.append(episode)\n",
    "    episode_json.append(json.dumps(episode))\n",
    "    file_name = agent.save(checkpoint_root)\n",
    "\n",
    "    print(f'{n:3d}: Min/Mean/Max reward: {result[\"episode_reward_min\"]:8.4f}/{result[\"episode_reward_mean\"]:8.4f}/{result[\"episode_reward_max\"]:8.4f}. Checkpoint saved to {file_name}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Preprocessor - Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-10 18:13:05.089593: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from preprocessing import AutoEncoder, Dataset\n",
    "from towerofhanoienv import TowerOfHanoiEnv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(\"Data/towerofHanoi_40k.npy\", batch_size=8)\n",
    "validation_dataset = Dataset(\"Data/towerofHanoi_10k.npy\", batch_size=2)\n",
    "#test_dataset = Dataset(\"Data/towerofHanoi_10k.npy\", batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "5000/5000 [==============================] - 14s 2ms/step - loss: 0.7304 - precision_26: 0.2408 - val_loss: 0.7319 - val_precision_26: 0.2500\n",
      "Epoch 2/50\n",
      "5000/5000 [==============================] - 12s 2ms/step - loss: 0.6971 - precision_26: 0.6217 - val_loss: 0.7152 - val_precision_26: 0.5000\n",
      "Epoch 3/50\n",
      "5000/5000 [==============================] - 12s 2ms/step - loss: 0.6761 - precision_26: 0.8992 - val_loss: 0.7064 - val_precision_26: 0.5000\n",
      "Epoch 4/50\n",
      "5000/5000 [==============================] - 12s 2ms/step - loss: 0.6619 - precision_26: 0.9460 - val_loss: 0.7054 - val_precision_26: 0.5455\n",
      "Epoch 5/50\n",
      "5000/5000 [==============================] - 12s 2ms/step - loss: 0.6537 - precision_26: 1.0000 - val_loss: 0.7102 - val_precision_26: 0.5455\n",
      "Epoch 6/50\n",
      "5000/5000 [==============================] - 12s 2ms/step - loss: 0.6480 - precision_26: 1.0000 - val_loss: 0.7175 - val_precision_26: 0.5455\n",
      "Epoch 7/50\n",
      "5000/5000 [==============================] - 12s 2ms/step - loss: 0.6438 - precision_26: 1.0000 - val_loss: 0.7272 - val_precision_26: 0.5455\n",
      "Epoch 8/50\n",
      "4990/5000 [============================>.] - ETA: 0s - loss: 0.6407 - precision_26: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/sophialawal/Visual_Studio/Deep_learning_projects/Reinforcement Learning/DQN/TowerofHanoi/TowerofHanoi.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sophialawal/Visual_Studio/Deep_learning_projects/Reinforcement%20Learning/DQN/TowerofHanoi/TowerofHanoi.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m AutoEncoder()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sophialawal/Visual_Studio/Deep_learning_projects/Reinforcement%20Learning/DQN/TowerofHanoi/TowerofHanoi.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(learning_rate\u001b[39m=\u001b[39m\u001b[39m1.2e-6\u001b[39m),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sophialawal/Visual_Studio/Deep_learning_projects/Reinforcement%20Learning/DQN/TowerofHanoi/TowerofHanoi.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m               loss\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlosses\u001b[39m.\u001b[39mBinaryCrossentropy(from_logits\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sophialawal/Visual_Studio/Deep_learning_projects/Reinforcement%20Learning/DQN/TowerofHanoi/TowerofHanoi.ipynb#X23sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m               metrics\u001b[39m=\u001b[39m[tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mPrecision()])\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sophialawal/Visual_Studio/Deep_learning_projects/Reinforcement%20Learning/DQN/TowerofHanoi/TowerofHanoi.ipynb#X23sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(train_dataset, epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49mvalidation_dataset, shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py:1694\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1679\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_eval_data_handler\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eval_data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39mget_data_handler(\n\u001b[1;32m   1681\u001b[0m         x\u001b[39m=\u001b[39mval_x,\n\u001b[1;32m   1682\u001b[0m         y\u001b[39m=\u001b[39mval_y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1692\u001b[0m         steps_per_execution\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution,\n\u001b[1;32m   1693\u001b[0m     )\n\u001b[0;32m-> 1694\u001b[0m val_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate(\n\u001b[1;32m   1695\u001b[0m     x\u001b[39m=\u001b[39;49mval_x,\n\u001b[1;32m   1696\u001b[0m     y\u001b[39m=\u001b[39;49mval_y,\n\u001b[1;32m   1697\u001b[0m     sample_weight\u001b[39m=\u001b[39;49mval_sample_weight,\n\u001b[1;32m   1698\u001b[0m     batch_size\u001b[39m=\u001b[39;49mvalidation_batch_size \u001b[39mor\u001b[39;49;00m batch_size,\n\u001b[1;32m   1699\u001b[0m     steps\u001b[39m=\u001b[39;49mvalidation_steps,\n\u001b[1;32m   1700\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m   1701\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[1;32m   1702\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[1;32m   1703\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[1;32m   1704\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1705\u001b[0m     _use_cached_eval_dataset\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1706\u001b[0m )\n\u001b[1;32m   1707\u001b[0m val_logs \u001b[39m=\u001b[39m {\n\u001b[1;32m   1708\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mval_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name: val \u001b[39mfor\u001b[39;00m name, val \u001b[39min\u001b[39;00m val_logs\u001b[39m.\u001b[39mitems()\n\u001b[1;32m   1709\u001b[0m }\n\u001b[1;32m   1710\u001b[0m epoch_logs\u001b[39m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py:2040\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   2036\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   2037\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m, step_num\u001b[39m=\u001b[39mstep, _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m   2038\u001b[0m ):\n\u001b[1;32m   2039\u001b[0m     callbacks\u001b[39m.\u001b[39mon_test_batch_begin(step)\n\u001b[0;32m-> 2040\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtest_function(iterator)\n\u001b[1;32m   2041\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   2042\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:919\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    917\u001b[0m \u001b[39m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    918\u001b[0m \u001b[39m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 919\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    920\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[1;32m    921\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCreating variables on a non-first call to a function\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    922\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39m decorated with tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:133\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[0;32m--> 133\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_function(args, kwargs)\n\u001b[1;32m    134\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39m_call_flat(\n\u001b[1;32m    135\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39mconcrete_function\u001b[39m.\u001b[39mcaptured_inputs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:324\u001b[0m, in \u001b[0;36mTracingCompiler._maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_maybe_define_function\u001b[39m(\u001b[39mself\u001b[39m, args, kwargs):\n\u001b[1;32m    304\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Gets a function for these inputs, defining it if necessary.\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \n\u001b[1;32m    306\u001b[0m \u001b[39m  Caller must hold self._lock.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[39m      shape relaxation retracing.\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m    323\u001b[0m   args, kwargs, filtered_flat_args \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 324\u001b[0m       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_function_spec\u001b[39m.\u001b[39;49mcanonicalize_function_inputs(args, kwargs))\n\u001b[1;32m    326\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_signature \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    327\u001b[0m     args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_signature\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/function_spec.py:536\u001b[0m, in \u001b[0;36mFunctionSpec.canonicalize_function_inputs\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    533\u001b[0m       kwargs\u001b[39m.\u001b[39msetdefault(kwarg, default)\n\u001b[1;32m    535\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_signature \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m   inputs, flat_inputs, filtered_flat_inputs \u001b[39m=\u001b[39m _convert_numpy_inputs(inputs)\n\u001b[1;32m    537\u001b[0m   kwargs, flat_kwargs, filtered_flat_kwargs \u001b[39m=\u001b[39m _convert_numpy_inputs(kwargs)\n\u001b[1;32m    538\u001b[0m   flat_inputs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m flat_kwargs\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/function_spec.py:625\u001b[0m, in \u001b[0;36m_convert_numpy_inputs\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_convert_numpy_inputs\u001b[39m(inputs):\n\u001b[1;32m    624\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Converts numpy array inputs to tensors.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 625\u001b[0m   flat_inputs \u001b[39m=\u001b[39m composite_tensor_utils\u001b[39m.\u001b[39;49mflatten_with_variables(inputs)\n\u001b[1;32m    627\u001b[0m   \u001b[39m# Check for NumPy arrays in arguments and convert them to Tensors.\u001b[39;00m\n\u001b[1;32m    628\u001b[0m   \u001b[39m# TODO(nareshmodi): Skip ndarray conversion to tensor altogether, perhaps\u001b[39;00m\n\u001b[1;32m    629\u001b[0m   \u001b[39m# finding a way to store them directly in the cache key (currently not\u001b[39;00m\n\u001b[1;32m    630\u001b[0m   \u001b[39m# possible since ndarrays are not hashable).\u001b[39;00m\n\u001b[1;32m    631\u001b[0m   need_packing \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/composite_tensor_utils.py:39\u001b[0m, in \u001b[0;36mflatten_with_variables\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39misinstance\u001b[39m(value, composite_tensor\u001b[39m.\u001b[39mCompositeTensor) \u001b[39mand\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     \u001b[39mnot\u001b[39;00m _pywrap_utils\u001b[39m.\u001b[39mIsResourceVariable(value)):\n\u001b[1;32m     38\u001b[0m   components \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39m_type_spec\u001b[39m.\u001b[39m_to_components(value)  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m   flat_inputs\u001b[39m.\u001b[39mextend(flatten_with_variables(components))\n\u001b[1;32m     40\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m   flat_inputs\u001b[39m.\u001b[39mappend(value)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/composite_tensor_utils.py:35\u001b[0m, in \u001b[0;36mflatten_with_variables\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39m# We assume that any CompositeTensors have already converted their components\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39m# from numpy arrays to Tensors, so we don't need to expand composites here for\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39m# the numpy array conversion. Instead, we do so because the flattened inputs\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39m# are eventually passed to ConcreteFunction()._call_flat, which requires\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[39m# expanded composites.\u001b[39;00m\n\u001b[1;32m     34\u001b[0m flat_inputs \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 35\u001b[0m \u001b[39mfor\u001b[39;00m value \u001b[39min\u001b[39;00m nest\u001b[39m.\u001b[39;49mflatten(inputs):\n\u001b[1;32m     36\u001b[0m   \u001b[39mif\u001b[39;00m (\u001b[39misinstance\u001b[39m(value, composite_tensor\u001b[39m.\u001b[39mCompositeTensor) \u001b[39mand\u001b[39;00m\n\u001b[1;32m     37\u001b[0m       \u001b[39mnot\u001b[39;00m _pywrap_utils\u001b[39m.\u001b[39mIsResourceVariable(value)):\n\u001b[1;32m     38\u001b[0m     components \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39m_type_spec\u001b[39m.\u001b[39m_to_components(value)  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/util/nest.py:454\u001b[0m, in \u001b[0;36mflatten\u001b[0;34m(structure, expand_composites)\u001b[0m\n\u001b[1;32m    452\u001b[0m   \u001b[39mreturn\u001b[39;00m [\u001b[39mNone\u001b[39;00m]\n\u001b[1;32m    453\u001b[0m expand_composites \u001b[39m=\u001b[39m \u001b[39mbool\u001b[39m(expand_composites)\n\u001b[0;32m--> 454\u001b[0m \u001b[39mreturn\u001b[39;00m _pywrap_utils\u001b[39m.\u001b[39;49mFlatten(structure, expand_composites)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = AutoEncoder()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1.2e-6),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=[tf.keras.metrics.Precision()])\n",
    "\n",
    "\n",
    "\n",
    "model.fit(train_dataset, epochs=50, validation_data=validation_dataset, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "weights = model.get_weights()\n",
    "with open('weights.pkl', 'wb') as f:\n",
    "    pickle.dump(weights, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "from preprocessing import AutoEncoder, Dataset\n",
    "from towerofhanoienv import TowerOfHanoiEnv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import ray\n",
    "\n",
    "class EmbeddingEnv(gymnasium.ObservationWrapper):\n",
    "# Override `observation` to custom process the original observation\n",
    "# coming from the env.\n",
    "    def __init__(self, env):\n",
    "         super().__init__(env)\n",
    "         self.observation_space = gymnasium.spaces.Box(shape=(8,), low=-np.inf, high=np.inf)\n",
    "    def observation(self, observation):\n",
    "        preprocess = ray.rllib.utils.numpy.one_hot(observation)\n",
    "        preprocess = preprocess.reshape(-1, 330)\n",
    "        model =  tf.keras.models.load_model('saved_model/autoencoder')\n",
    "        output = model.encoder(preprocess)\n",
    "        print(output)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "def env_creator(env_config):\n",
    "    return EmbeddingEnv(TowerOfHanoiEnv(env_config))\n",
    "\n",
    "register_env(\"TowerOfHanoiEnv\", env_creator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-10 23:07:20,865\tWARNING env.py:156 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.\n",
      "2023-03-10 23:07:20,866\tWARNING env.py:166 -- Your env reset() method appears to take 'seed' or 'return_info' arguments. Note that these are not yet supported in RLlib. Seeding will take place using 'env.seed()' and the info dict will not be returned from reset.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/sophialawal/Visual_Studio/Deep_learning_projects/Reinforcement Learning/DQN/TowerofHanoi/TowerofHanoi.ipynb Cell 20\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sophialawal/Visual_Studio/Deep_learning_projects/Reinforcement%20Learning/DQN/TowerofHanoi/TowerofHanoi.ipynb#X26sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtowerofhanoienv\u001b[39;00m \u001b[39mimport\u001b[39;00m TowerOfHanoiEnv\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sophialawal/Visual_Studio/Deep_learning_projects/Reinforcement%20Learning/DQN/TowerofHanoi/TowerofHanoi.ipynb#X26sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m checkpoint_root \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtmp/dqn/towerofhanoi\u001b[39m\u001b[39m\"\u001b[39m;\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sophialawal/Visual_Studio/Deep_learning_projects/Reinforcement%20Learning/DQN/TowerofHanoi/TowerofHanoi.ipynb#X26sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m algo \u001b[39m=\u001b[39m DQNConfig()\u001b[39m.\u001b[39;49menvironment(TowerOfHanoiEnv, env_config\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mnum_disks\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m5\u001b[39;49m})\u001b[39m.\u001b[39;49mtraining(model\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39m_disable_preprocessor_api\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mTrue\u001b[39;49;00m,\u001b[39m\"\u001b[39;49m\u001b[39muse_lstm\u001b[39;49m\u001b[39m\"\u001b[39;49m:\u001b[39mTrue\u001b[39;49;00m}, lr\u001b[39m=\u001b[39;49m\u001b[39m0.01\u001b[39;49m)\u001b[39m.\u001b[39;49mbuild()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sophialawal/Visual_Studio/Deep_learning_projects/Reinforcement%20Learning/DQN/TowerofHanoi/TowerofHanoi.ipynb#X26sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m#algo = DQNConfig().environment(TowerOfHanoiEnv, env_config={\"num_disks\": 5}).rollouts(num_rollout_workers=1).training(model={\"fcnet_hiddens\":[256,128,64], \"fcnet_activation\":\"tanh\", \"_disable_preprocessor_api\": True,\"use_attention\":True}, lr=0.01).build()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sophialawal/Visual_Studio/Deep_learning_projects/Reinforcement%20Learning/DQN/TowerofHanoi/TowerofHanoi.ipynb#X26sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m results \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm_config.py:926\u001b[0m, in \u001b[0;36mAlgorithmConfig.build\u001b[0;34m(self, env, logger_creator, use_copy)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39malgo_class, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    924\u001b[0m     algo_class \u001b[39m=\u001b[39m get_trainable_cls(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39malgo_class)\n\u001b[0;32m--> 926\u001b[0m \u001b[39mreturn\u001b[39;00m algo_class(\n\u001b[1;32m    927\u001b[0m     config\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m use_copy \u001b[39melse\u001b[39;49;00m copy\u001b[39m.\u001b[39;49mdeepcopy(\u001b[39mself\u001b[39;49m),\n\u001b[1;32m    928\u001b[0m     logger_creator\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlogger_creator,\n\u001b[1;32m    929\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:445\u001b[0m, in \u001b[0;36mAlgorithm.__init__\u001b[0;34m(self, config, env, logger_creator, **kwargs)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[39m# Initialize common evaluation_metrics to nan, before they become\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[39m# available. We want to make sure the metrics are always present\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[39m# (although their values may be nan), so that Tune does not complain\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[39m# when we use these as stopping criteria.\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_metrics \u001b[39m=\u001b[39m {\n\u001b[1;32m    438\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mevaluation\u001b[39m\u001b[39m\"\u001b[39m: {\n\u001b[1;32m    439\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mepisode_reward_max\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mnan,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    442\u001b[0m     }\n\u001b[1;32m    443\u001b[0m }\n\u001b[0;32m--> 445\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    446\u001b[0m     config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    447\u001b[0m     logger_creator\u001b[39m=\u001b[39;49mlogger_creator,\n\u001b[1;32m    448\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    449\u001b[0m )\n\u001b[1;32m    451\u001b[0m \u001b[39m# Check, whether `training_iteration` is still a tune.Trainable property\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \u001b[39m# and has not been overridden by the user in the attempt to implement the\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[39m# algos logic (this should be done now inside `training_step`).\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ray/tune/trainable/trainable.py:169\u001b[0m, in \u001b[0;36mTrainable.__init__\u001b[0;34m(self, config, logger_creator, remote_checkpoint_dir, custom_syncer, sync_timeout)\u001b[0m\n\u001b[1;32m    167\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    168\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_local_ip \u001b[39m=\u001b[39m ray\u001b[39m.\u001b[39mutil\u001b[39m.\u001b[39mget_node_ip_address()\n\u001b[0;32m--> 169\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msetup(copy\u001b[39m.\u001b[39;49mdeepcopy(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig))\n\u001b[1;32m    170\u001b[0m setup_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n\u001b[1;32m    171\u001b[0m \u001b[39mif\u001b[39;00m setup_time \u001b[39m>\u001b[39m SETUP_TIME_THRESHOLD:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:571\u001b[0m, in \u001b[0;36mAlgorithm.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[39m# Only if user did not override `_init()`:\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[39mif\u001b[39;00m _init \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    566\u001b[0m     \u001b[39m# - Create rollout workers here automatically.\u001b[39;00m\n\u001b[1;32m    567\u001b[0m     \u001b[39m# - Run the execution plan to create the local iterator to `next()`\u001b[39;00m\n\u001b[1;32m    568\u001b[0m     \u001b[39m#   in each training iteration.\u001b[39;00m\n\u001b[1;32m    569\u001b[0m     \u001b[39m# This matches the behavior of using `build_trainer()`, which\u001b[39;00m\n\u001b[1;32m    570\u001b[0m     \u001b[39m# has been deprecated.\u001b[39;00m\n\u001b[0;32m--> 571\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkers \u001b[39m=\u001b[39m WorkerSet(\n\u001b[1;32m    572\u001b[0m         env_creator\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv_creator,\n\u001b[1;32m    573\u001b[0m         validate_env\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalidate_env,\n\u001b[1;32m    574\u001b[0m         default_policy_class\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_default_policy_class(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig),\n\u001b[1;32m    575\u001b[0m         config\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig,\n\u001b[1;32m    576\u001b[0m         num_workers\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mnum_rollout_workers,\n\u001b[1;32m    577\u001b[0m         local_worker\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    578\u001b[0m         logdir\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlogdir,\n\u001b[1;32m    579\u001b[0m     )\n\u001b[1;32m    581\u001b[0m     \u001b[39m# TODO (avnishn): Remove the execution plan API by q1 2023\u001b[39;00m\n\u001b[1;32m    582\u001b[0m     \u001b[39m# Function defining one single training iteration's behavior.\u001b[39;00m\n\u001b[1;32m    583\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39m_disable_execution_plan_api:\n\u001b[1;32m    584\u001b[0m         \u001b[39m# Ensure remote workers are initially in sync with the local worker.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py:170\u001b[0m, in \u001b[0;36mWorkerSet.__init__\u001b[0;34m(self, env_creator, validate_env, default_policy_class, config, num_workers, local_worker, logdir, _setup, policy_class, trainer_config)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[39mif\u001b[39;00m _setup:\n\u001b[1;32m    169\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setup(\n\u001b[1;32m    171\u001b[0m             validate_env\u001b[39m=\u001b[39;49mvalidate_env,\n\u001b[1;32m    172\u001b[0m             config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    173\u001b[0m             num_workers\u001b[39m=\u001b[39;49mnum_workers,\n\u001b[1;32m    174\u001b[0m             local_worker\u001b[39m=\u001b[39;49mlocal_worker,\n\u001b[1;32m    175\u001b[0m         )\n\u001b[1;32m    176\u001b[0m     \u001b[39m# WorkerSet creation possibly fails, if some (remote) workers cannot\u001b[39;00m\n\u001b[1;32m    177\u001b[0m     \u001b[39m# be initialized properly (due to some errors in the RolloutWorker's\u001b[39;00m\n\u001b[1;32m    178\u001b[0m     \u001b[39m# constructor).\u001b[39;00m\n\u001b[1;32m    179\u001b[0m     \u001b[39mexcept\u001b[39;00m RayActorError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    180\u001b[0m         \u001b[39m# In case of an actor (remote worker) init failure, the remote worker\u001b[39;00m\n\u001b[1;32m    181\u001b[0m         \u001b[39m# may still exist and will be accessible, however, e.g. calling\u001b[39;00m\n\u001b[1;32m    182\u001b[0m         \u001b[39m# its `sample.remote()` would result in strange \"property not found\"\u001b[39;00m\n\u001b[1;32m    183\u001b[0m         \u001b[39m# errors.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py:260\u001b[0m, in \u001b[0;36mWorkerSet._setup\u001b[0;34m(self, validate_env, config, num_workers, local_worker)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[39m# Create a local worker, if needed.\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[39mif\u001b[39;00m local_worker:\n\u001b[0;32m--> 260\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_local_worker \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_worker(\n\u001b[1;32m    261\u001b[0m         \u001b[39mcls\u001b[39;49m\u001b[39m=\u001b[39;49mRolloutWorker,\n\u001b[1;32m    262\u001b[0m         env_creator\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_env_creator,\n\u001b[1;32m    263\u001b[0m         validate_env\u001b[39m=\u001b[39;49mvalidate_env,\n\u001b[1;32m    264\u001b[0m         worker_index\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m    265\u001b[0m         num_workers\u001b[39m=\u001b[39;49mnum_workers,\n\u001b[1;32m    266\u001b[0m         config\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_local_config,\n\u001b[1;32m    267\u001b[0m         spaces\u001b[39m=\u001b[39;49mspaces,\n\u001b[1;32m    268\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py:946\u001b[0m, in \u001b[0;36mWorkerSet._make_worker\u001b[0;34m(self, cls, env_creator, validate_env, worker_index, num_workers, recreated_worker, config, spaces)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_make_worker\u001b[39m(\n\u001b[1;32m    933\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    934\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     ] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    945\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[RolloutWorker, ActorHandle]:\n\u001b[0;32m--> 946\u001b[0m     worker \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(\n\u001b[1;32m    947\u001b[0m         env_creator\u001b[39m=\u001b[39;49menv_creator,\n\u001b[1;32m    948\u001b[0m         validate_env\u001b[39m=\u001b[39;49mvalidate_env,\n\u001b[1;32m    949\u001b[0m         default_policy_class\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_policy_class,\n\u001b[1;32m    950\u001b[0m         config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    951\u001b[0m         worker_index\u001b[39m=\u001b[39;49mworker_index,\n\u001b[1;32m    952\u001b[0m         num_workers\u001b[39m=\u001b[39;49mnum_workers,\n\u001b[1;32m    953\u001b[0m         recreated_worker\u001b[39m=\u001b[39;49mrecreated_worker,\n\u001b[1;32m    954\u001b[0m         log_dir\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_logdir,\n\u001b[1;32m    955\u001b[0m         spaces\u001b[39m=\u001b[39;49mspaces,\n\u001b[1;32m    956\u001b[0m         dataset_shards\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ds_shards,\n\u001b[1;32m    957\u001b[0m     )\n\u001b[1;32m    959\u001b[0m     \u001b[39mreturn\u001b[39;00m worker\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py:737\u001b[0m, in \u001b[0;36mRolloutWorker.__init__\u001b[0;34m(self, env_creator, validate_env, config, worker_index, num_workers, recreated_worker, log_dir, spaces, default_policy_class, dataset_shards, policy_config, input_creator, output_creator, rollout_fragment_length, count_steps_by, batch_mode, episode_horizon, preprocessor_pref, sample_async, compress_observations, num_envs, observation_fn, clip_rewards, normalize_actions, clip_actions, env_config, model_config, remote_worker_envs, remote_env_batch_wait_ms, soft_horizon, no_done_at_end, fake_sampler, seed, log_level, callbacks, disable_env_checking, policy_spec, policy_mapping_fn, policies_to_train, extra_python_environs, policy, tf_session_creator)\u001b[0m\n\u001b[1;32m    728\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    729\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are running ray with `local_mode=True`, but have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    730\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mconfigured \u001b[39m\u001b[39m{\u001b[39;00mnum_gpus\u001b[39m}\u001b[39;00m\u001b[39m GPUs to be used! In local mode, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    731\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPolicies are placed on the CPU and the `num_gpus` setting \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    732\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mis ignored.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    733\u001b[0m     )\n\u001b[1;32m    735\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilters: Dict[PolicyID, Filter] \u001b[39m=\u001b[39m defaultdict(NoFilter)\n\u001b[0;32m--> 737\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_build_policy_map(policy_dict\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy_dict)\n\u001b[1;32m    739\u001b[0m \u001b[39m# Update Policy's view requirements from Model, only if Policy directly\u001b[39;00m\n\u001b[1;32m    740\u001b[0m \u001b[39m# inherited from base `Policy` class. At this point here, the Policy\u001b[39;00m\n\u001b[1;32m    741\u001b[0m \u001b[39m# must have it's Model (if any) defined and ready to output an initial\u001b[39;00m\n\u001b[1;32m    742\u001b[0m \u001b[39m# state.\u001b[39;00m\n\u001b[1;32m    743\u001b[0m \u001b[39mfor\u001b[39;00m pol \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy_map\u001b[39m.\u001b[39mvalues():\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py:1984\u001b[0m, in \u001b[0;36mRolloutWorker._build_policy_map\u001b[0;34m(self, policy_dict, policy, policy_states)\u001b[0m\n\u001b[1;32m   1982\u001b[0m \u001b[39m# Create the actual policy object.\u001b[39;00m\n\u001b[1;32m   1983\u001b[0m \u001b[39mif\u001b[39;00m policy \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1984\u001b[0m     new_policy \u001b[39m=\u001b[39m create_policy_for_framework(\n\u001b[1;32m   1985\u001b[0m         policy_id\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m   1986\u001b[0m         policy_class\u001b[39m=\u001b[39;49mget_tf_eager_cls_if_necessary(\n\u001b[1;32m   1987\u001b[0m             policy_spec\u001b[39m.\u001b[39;49mpolicy_class, merged_conf\n\u001b[1;32m   1988\u001b[0m         ),\n\u001b[1;32m   1989\u001b[0m         merged_config\u001b[39m=\u001b[39;49mmerged_conf,\n\u001b[1;32m   1990\u001b[0m         observation_space\u001b[39m=\u001b[39;49mobs_space,\n\u001b[1;32m   1991\u001b[0m         action_space\u001b[39m=\u001b[39;49mpolicy_spec\u001b[39m.\u001b[39;49maction_space,\n\u001b[1;32m   1992\u001b[0m         worker_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mworker_index,\n\u001b[1;32m   1993\u001b[0m         seed\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseed,\n\u001b[1;32m   1994\u001b[0m     )\n\u001b[1;32m   1995\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1996\u001b[0m     new_policy \u001b[39m=\u001b[39m policy\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ray/rllib/utils/policy.py:130\u001b[0m, in \u001b[0;36mcreate_policy_for_framework\u001b[0;34m(policy_id, policy_class, merged_config, observation_space, action_space, worker_index, session_creator, seed)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 tf1\u001b[39m.\u001b[39mset_random_seed(seed)\n\u001b[1;32m    129\u001b[0m             \u001b[39mwith\u001b[39;00m tf1\u001b[39m.\u001b[39mvariable_scope(var_scope):\n\u001b[0;32m--> 130\u001b[0m                 \u001b[39mreturn\u001b[39;00m policy_class(\n\u001b[1;32m    131\u001b[0m                     observation_space, action_space, merged_config\n\u001b[1;32m    132\u001b[0m                 )\n\u001b[1;32m    133\u001b[0m \u001b[39m# For tf-eager: no graph, no session.\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m     \u001b[39mwith\u001b[39;00m tf1\u001b[39m.\u001b[39mvariable_scope(var_scope):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ray/rllib/policy/tf_policy_template.py:258\u001b[0m, in \u001b[0;36mbuild_tf_policy.<locals>.policy_cls.__init__\u001b[0;34m(self, obs_space, action_space, config, existing_model, existing_inputs)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    256\u001b[0m         policy\u001b[39m.\u001b[39m_extra_action_fetches \u001b[39m=\u001b[39m extra_action_fetches\n\u001b[0;32m--> 258\u001b[0m DynamicTFPolicy\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    259\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[1;32m    260\u001b[0m     obs_space\u001b[39m=\u001b[39;49mobs_space,\n\u001b[1;32m    261\u001b[0m     action_space\u001b[39m=\u001b[39;49maction_space,\n\u001b[1;32m    262\u001b[0m     config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    263\u001b[0m     loss_fn\u001b[39m=\u001b[39;49mloss_fn,\n\u001b[1;32m    264\u001b[0m     stats_fn\u001b[39m=\u001b[39;49mstats_fn,\n\u001b[1;32m    265\u001b[0m     grad_stats_fn\u001b[39m=\u001b[39;49mgrad_stats_fn,\n\u001b[1;32m    266\u001b[0m     before_loss_init\u001b[39m=\u001b[39;49mbefore_loss_init_wrapper,\n\u001b[1;32m    267\u001b[0m     make_model\u001b[39m=\u001b[39;49mmake_model,\n\u001b[1;32m    268\u001b[0m     action_sampler_fn\u001b[39m=\u001b[39;49maction_sampler_fn,\n\u001b[1;32m    269\u001b[0m     action_distribution_fn\u001b[39m=\u001b[39;49maction_distribution_fn,\n\u001b[1;32m    270\u001b[0m     existing_inputs\u001b[39m=\u001b[39;49mexisting_inputs,\n\u001b[1;32m    271\u001b[0m     existing_model\u001b[39m=\u001b[39;49mexisting_model,\n\u001b[1;32m    272\u001b[0m     get_batch_divisibility_req\u001b[39m=\u001b[39;49mget_batch_divisibility_req,\n\u001b[1;32m    273\u001b[0m )\n\u001b[1;32m    275\u001b[0m \u001b[39mif\u001b[39;00m after_init:\n\u001b[1;32m    276\u001b[0m     after_init(\u001b[39mself\u001b[39m, obs_space, action_space, config)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ray/rllib/policy/dynamic_tf_policy.py:462\u001b[0m, in \u001b[0;36mDynamicTFPolicy.__init__\u001b[0;34m(self, obs_space, action_space, config, loss_fn, stats_fn, grad_stats_fn, before_loss_init, make_model, action_sampler_fn, action_distribution_fn, existing_inputs, existing_model, get_batch_divisibility_req, obs_include_prev_action_reward)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[39m# Loss initialization and model/postprocessing test calls.\u001b[39;00m\n\u001b[1;32m    461\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_tower:\n\u001b[0;32m--> 462\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_initialize_loss_from_dummy_batch(auto_remove_unneeded_view_reqs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    464\u001b[0m     \u001b[39m# Create MultiGPUTowerStacks, if we have at least one actual\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     \u001b[39m# GPU or >1 CPUs (fake GPUs).\u001b[39;00m\n\u001b[1;32m    466\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevices) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39many\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mgpu\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m d \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevices):\n\u001b[1;32m    467\u001b[0m         \u001b[39m# Per-GPU graph copies created here must share vars with the\u001b[39;00m\n\u001b[1;32m    468\u001b[0m         \u001b[39m# policy. Therefore, `reuse` is set to tf1.AUTO_REUSE because\u001b[39;00m\n\u001b[1;32m    469\u001b[0m         \u001b[39m# Adam nodes are created after all of the device copies are\u001b[39;00m\n\u001b[1;32m    470\u001b[0m         \u001b[39m# created.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ray/rllib/policy/dynamic_tf_policy.py:797\u001b[0m, in \u001b[0;36mDynamicTFPolicy._initialize_loss_from_dummy_batch\u001b[0;34m(self, auto_remove_unneeded_view_reqs, stats_fn)\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[39mif\u001b[39;00m log_once(\u001b[39m\"\u001b[39m\u001b[39mloss_init\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    791\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\n\u001b[1;32m    792\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mInitializing loss function with dummy input:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    793\u001b[0m             summarize(train_batch)\n\u001b[1;32m    794\u001b[0m         )\n\u001b[1;32m    795\u001b[0m     )\n\u001b[0;32m--> 797\u001b[0m losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_loss_init(train_batch)\n\u001b[1;32m    799\u001b[0m all_accessed_keys \u001b[39m=\u001b[39m (\n\u001b[1;32m    800\u001b[0m     train_batch\u001b[39m.\u001b[39maccessed_keys\n\u001b[1;32m    801\u001b[0m     \u001b[39m|\u001b[39m dummy_batch\u001b[39m.\u001b[39maccessed_keys\n\u001b[1;32m    802\u001b[0m     \u001b[39m|\u001b[39m dummy_batch\u001b[39m.\u001b[39madded_keys\n\u001b[1;32m    803\u001b[0m     \u001b[39m|\u001b[39m \u001b[39mset\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mview_requirements\u001b[39m.\u001b[39mkeys())\n\u001b[1;32m    804\u001b[0m )\n\u001b[1;32m    806\u001b[0m TFPolicy\u001b[39m.\u001b[39m_initialize_loss(\n\u001b[1;32m    807\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    808\u001b[0m     losses,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    814\u001b[0m     ),\n\u001b[1;32m    815\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ray/rllib/policy/dynamic_tf_policy.py:911\u001b[0m, in \u001b[0;36mDynamicTFPolicy._do_loss_init\u001b[0;34m(self, train_batch)\u001b[0m\n\u001b[1;32m    910\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_do_loss_init\u001b[39m(\u001b[39mself\u001b[39m, train_batch: SampleBatch):\n\u001b[0;32m--> 911\u001b[0m     losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_loss_fn(\u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdist_class, train_batch)\n\u001b[1;32m    912\u001b[0m     losses \u001b[39m=\u001b[39m force_list(losses)\n\u001b[1;32m    913\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stats_fn:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ray/rllib/algorithms/dqn/dqn_tf_policy.py:259\u001b[0m, in \u001b[0;36mbuild_q_losses\u001b[0;34m(policy, model, _, train_batch)\u001b[0m\n\u001b[1;32m    257\u001b[0m config \u001b[39m=\u001b[39m policy\u001b[39m.\u001b[39mconfig\n\u001b[1;32m    258\u001b[0m \u001b[39m# q network evaluation\u001b[39;00m\n\u001b[0;32m--> 259\u001b[0m q_t, q_logits_t, q_dist_t, _ \u001b[39m=\u001b[39m compute_q_values(\n\u001b[1;32m    260\u001b[0m     policy,\n\u001b[1;32m    261\u001b[0m     model,\n\u001b[1;32m    262\u001b[0m     SampleBatch({\u001b[39m\"\u001b[39;49m\u001b[39mobs\u001b[39;49m\u001b[39m\"\u001b[39;49m: train_batch[SampleBatch\u001b[39m.\u001b[39;49mCUR_OBS]}),\n\u001b[1;32m    263\u001b[0m     state_batches\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    264\u001b[0m     explore\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    265\u001b[0m )\n\u001b[1;32m    267\u001b[0m \u001b[39m# target q network evalution\u001b[39;00m\n\u001b[1;32m    268\u001b[0m q_tp1, q_logits_tp1, q_dist_tp1, _ \u001b[39m=\u001b[39m compute_q_values(\n\u001b[1;32m    269\u001b[0m     policy,\n\u001b[1;32m    270\u001b[0m     policy\u001b[39m.\u001b[39mtarget_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    273\u001b[0m     explore\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    274\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ray/rllib/algorithms/dqn/dqn_tf_policy.py:401\u001b[0m, in \u001b[0;36mcompute_q_values\u001b[0;34m(policy, model, input_batch, state_batches, seq_lens, explore, is_training)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_q_values\u001b[39m(\n\u001b[1;32m    390\u001b[0m     policy: Policy,\n\u001b[1;32m    391\u001b[0m     model: ModelV2,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m     is_training: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    397\u001b[0m ):\n\u001b[1;32m    399\u001b[0m     config \u001b[39m=\u001b[39m policy\u001b[39m.\u001b[39mconfig\n\u001b[0;32m--> 401\u001b[0m     model_out, state \u001b[39m=\u001b[39m model(input_batch, state_batches \u001b[39mor\u001b[39;49;00m [], seq_lens)\n\u001b[1;32m    403\u001b[0m     \u001b[39mif\u001b[39;00m config[\u001b[39m\"\u001b[39m\u001b[39mnum_atoms\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    404\u001b[0m         (\n\u001b[1;32m    405\u001b[0m             action_scores,\n\u001b[1;32m    406\u001b[0m             z,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    409\u001b[0m             dist,\n\u001b[1;32m    410\u001b[0m         ) \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mget_q_value_distributions(model_out)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ray/rllib/models/modelv2.py:259\u001b[0m, in \u001b[0;36mModelV2.__call__\u001b[0;34m(self, input_dict, state, seq_lens)\u001b[0m\n\u001b[1;32m    256\u001b[0m         restored[\u001b[39m\"\u001b[39m\u001b[39mobs_flat\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m input_dict[\u001b[39m\"\u001b[39m\u001b[39mobs\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    258\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext():\n\u001b[0;32m--> 259\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(restored, state \u001b[39mor\u001b[39;49;00m [], seq_lens)\n\u001b[1;32m    261\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(input_dict, SampleBatch):\n\u001b[1;32m    262\u001b[0m     input_dict\u001b[39m.\u001b[39maccessed_keys \u001b[39m=\u001b[39m restored\u001b[39m.\u001b[39maccessed_keys \u001b[39m-\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mobs_flat\u001b[39m\u001b[39m\"\u001b[39m}\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ray/rllib/models/tf/recurrent_net.py:222\u001b[0m, in \u001b[0;36mLSTMWrapper.forward\u001b[0;34m(self, input_dict, state, seq_lens)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39m@override\u001b[39m(RecurrentNetwork)\n\u001b[1;32m    216\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    217\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    220\u001b[0m     seq_lens: TensorType,\n\u001b[1;32m    221\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[TensorType, List[TensorType]]:\n\u001b[0;32m--> 222\u001b[0m     \u001b[39massert\u001b[39;00m seq_lens \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[39m# Push obs through \"unwrapped\" net's `forward()` first.\u001b[39;00m\n\u001b[1;32m    224\u001b[0m     wrapped_out, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrapped_forward(input_dict, [], \u001b[39mNone\u001b[39;00m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.dqn.dqn import DQNConfig\n",
    "from ray.tune.logger import pretty_print\n",
    "import tensorflow as tf\n",
    "import json\n",
    "\n",
    "from towerofhanoienv import TowerOfHanoiEnv\n",
    "\n",
    "\n",
    "checkpoint_root = \"tmp/dqn/towerofhanoi\";\n",
    "algo = DQNConfig().environment(TowerOfHanoiEnv, env_config={\"num_disks\": 5}).training(model={\"_disable_preprocessor_api\": True,\"use_lstm\":True}, lr=0.01).build()\n",
    "#algo = DQNConfig().environment(TowerOfHanoiEnv, env_config={\"num_disks\": 5}).rollouts(num_rollout_workers=1).training(model={\"fcnet_hiddens\":[256,128,64], \"fcnet_activation\":\"tanh\", \"_disable_preprocessor_api\": True,\"use_attention\":True}, lr=0.01).build()\n",
    "\n",
    "results = []\n",
    "episode_data = []\n",
    "episode_json = []\n",
    "\n",
    "for n in range(5):\n",
    "    result = algo.train()\n",
    "    results.append(result)\n",
    "\n",
    "    episode = {\n",
    "        \"n\": n,\n",
    "        \"episode_reward_min\": result[\"episode_reward_min\"],\n",
    "        \"episode_reward_mean\": result[\"episode_reward_mean\"],\n",
    "        \"episode_reward_max\": result[\"episode_reward_max\"],\n",
    "        \"episode_len_mean\": result[\"episode_len_mean\"],\n",
    "    }\n",
    "\n",
    "    episode_data.append(episode)\n",
    "    episode_json.append(json.dumps(episode))\n",
    "    file_name = algo.save(checkpoint_root)\n",
    "\n",
    "    print(f'{n:3d}: Min/Mean/Max reward: {result[\"episode_reward_min\"]:8.4f}/{result[\"episode_reward_mean\"]:8.4f}/{result[\"episode_reward_max\"]:8.4f}. Checkpoint saved to {file_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'custom_metrics': {},\n",
       " 'episode_media': {},\n",
       " 'info': {'learner': {'default_policy': {'learner_stats': {'cur_lr': 0.009999999776482582,\n",
       "     'mean_q': -7.8514757,\n",
       "     'min_q': -8.262798,\n",
       "     'max_q': -7.2481847,\n",
       "     'mean_td_error': -0.034461588,\n",
       "     'model': {}},\n",
       "    'td_error': array([ 0.04377365, -0.19527864, -0.20345926, -0.01094389,  0.13382912,\n",
       "           -0.16710758, -0.05218697, -0.2419281 , -0.12647867,  0.11384296,\n",
       "           -0.10818195,  0.13382912,  0.00939035, -0.17485619, -0.17335749,\n",
       "            0.02789545, -0.18329859,  0.10252094, -0.0297308 , -0.06614017,\n",
       "            0.04377365,  0.00196362,  0.02789545, -0.03977776,  0.02482843,\n",
       "           -0.11675215, -0.05057478, -0.09322596,  0.0444994 ,  0.02554703,\n",
       "           -0.19767904,  0.394598  ], dtype=float32),\n",
       "    'custom_metrics': {},\n",
       "    'num_agent_steps_trained': 32.0,\n",
       "    'num_grad_updates_lifetime': 3000.0,\n",
       "    'diff_num_grad_updates_vs_sampler_policy': 2999.0}},\n",
       "  'num_env_steps_sampled': 4000,\n",
       "  'num_env_steps_trained': 96000,\n",
       "  'num_agent_steps_sampled': 4000,\n",
       "  'num_agent_steps_trained': 96000,\n",
       "  'last_target_update_ts': 3501,\n",
       "  'num_target_updates': 6},\n",
       " 'sampler_results': {'episode_reward_max': nan,\n",
       "  'episode_reward_min': nan,\n",
       "  'episode_reward_mean': nan,\n",
       "  'episode_len_mean': nan,\n",
       "  'episode_media': {},\n",
       "  'episodes_this_iter': 0,\n",
       "  'policy_reward_min': {},\n",
       "  'policy_reward_max': {},\n",
       "  'policy_reward_mean': {},\n",
       "  'custom_metrics': {},\n",
       "  'hist_stats': {'episode_reward': [], 'episode_lengths': []},\n",
       "  'sampler_perf': {},\n",
       "  'num_faulty_episodes': 0,\n",
       "  'connector_metrics': {}},\n",
       " 'episode_reward_max': nan,\n",
       " 'episode_reward_min': nan,\n",
       " 'episode_reward_mean': nan,\n",
       " 'episode_len_mean': nan,\n",
       " 'episodes_this_iter': 0,\n",
       " 'policy_reward_min': {},\n",
       " 'policy_reward_max': {},\n",
       " 'policy_reward_mean': {},\n",
       " 'hist_stats': {'episode_reward': [], 'episode_lengths': []},\n",
       " 'sampler_perf': {},\n",
       " 'num_faulty_episodes': 0,\n",
       " 'connector_metrics': {},\n",
       " 'num_healthy_workers': 1,\n",
       " 'num_in_flight_async_reqs': 0,\n",
       " 'num_remote_worker_restarts': 0,\n",
       " 'num_agent_steps_sampled': 4000,\n",
       " 'num_agent_steps_trained': 96000,\n",
       " 'num_env_steps_sampled': 4000,\n",
       " 'num_env_steps_trained': 96000,\n",
       " 'num_env_steps_sampled_this_iter': 1000,\n",
       " 'num_env_steps_trained_this_iter': 32000,\n",
       " 'timesteps_total': 4000,\n",
       " 'num_steps_trained_this_iter': 32000,\n",
       " 'agent_timesteps_total': 4000,\n",
       " 'timers': {'training_iteration_time_ms': 38.216,\n",
       "  'load_time_ms': 0.234,\n",
       "  'load_throughput': 136984.821,\n",
       "  'learn_time_ms': 6.414,\n",
       "  'learn_throughput': 4988.858,\n",
       "  'synch_weights_time_ms': 2.897},\n",
       " 'counters': {'num_env_steps_sampled': 4000,\n",
       "  'num_env_steps_trained': 96000,\n",
       "  'num_agent_steps_sampled': 4000,\n",
       "  'num_agent_steps_trained': 96000,\n",
       "  'last_target_update_ts': 3501,\n",
       "  'num_target_updates': 6},\n",
       " 'done': False,\n",
       " 'episodes_total': 0,\n",
       " 'training_iteration': 4,\n",
       " 'trial_id': 'default',\n",
       " 'experiment_id': '0b7e1eba71ff4c23a1fa92fa6c88ffa1',\n",
       " 'date': '2023-03-10_22-58-48',\n",
       " 'timestamp': 1678489128,\n",
       " 'time_this_iter_s': 37.06512427330017,\n",
       " 'time_total_s': 112.43479704856873,\n",
       " 'pid': 38974,\n",
       " 'hostname': 'Sophias-MBP.lan',\n",
       " 'node_ip': '127.0.0.1',\n",
       " 'config': {'extra_python_environs_for_driver': {},\n",
       "  'extra_python_environs_for_worker': {},\n",
       "  'num_gpus': 0,\n",
       "  'num_cpus_per_worker': 1,\n",
       "  'num_gpus_per_worker': 0,\n",
       "  '_fake_gpus': False,\n",
       "  'num_trainer_workers': 0,\n",
       "  'num_gpus_per_trainer_worker': 0,\n",
       "  'num_cpus_per_trainer_worker': 1,\n",
       "  'custom_resources_per_worker': {},\n",
       "  'placement_strategy': 'PACK',\n",
       "  'eager_tracing': False,\n",
       "  'eager_max_retraces': 20,\n",
       "  'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "   'inter_op_parallelism_threads': 2,\n",
       "   'gpu_options': {'allow_growth': True},\n",
       "   'log_device_placement': False,\n",
       "   'device_count': {'CPU': 1},\n",
       "   'allow_soft_placement': True},\n",
       "  'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "   'inter_op_parallelism_threads': 8},\n",
       "  'env': towerofhanoienv.TowerOfHanoiEnv,\n",
       "  'env_config': {'num_disks': 5},\n",
       "  'observation_space': None,\n",
       "  'action_space': None,\n",
       "  'env_task_fn': None,\n",
       "  'render_env': False,\n",
       "  'clip_rewards': None,\n",
       "  'normalize_actions': True,\n",
       "  'clip_actions': False,\n",
       "  'disable_env_checking': False,\n",
       "  'is_atari': False,\n",
       "  'auto_wrap_old_gym_envs': True,\n",
       "  'num_envs_per_worker': 1,\n",
       "  'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       "  'sample_async': False,\n",
       "  'enable_connectors': True,\n",
       "  'rollout_fragment_length': 'auto',\n",
       "  'batch_mode': 'truncate_episodes',\n",
       "  'remote_worker_envs': False,\n",
       "  'remote_env_batch_wait_ms': 0,\n",
       "  'validate_workers_after_construction': True,\n",
       "  'ignore_worker_failures': False,\n",
       "  'recreate_failed_workers': False,\n",
       "  'restart_failed_sub_environments': False,\n",
       "  'num_consecutive_worker_failures_tolerance': 100,\n",
       "  'preprocessor_pref': 'deepmind',\n",
       "  'observation_filter': 'NoFilter',\n",
       "  'synchronize_filters': True,\n",
       "  'compress_observations': False,\n",
       "  'enable_tf1_exec_eagerly': False,\n",
       "  'sampler_perf_stats_ema_coef': None,\n",
       "  'worker_health_probe_timeout_s': 60,\n",
       "  'worker_restore_timeout_s': 1800,\n",
       "  'gamma': 0.99,\n",
       "  'lr': 0.01,\n",
       "  'train_batch_size': 32,\n",
       "  'model': {'_disable_preprocessor_api': True,\n",
       "   '_disable_action_flattening': False,\n",
       "   'fcnet_hiddens': [512, 256, 128, 64],\n",
       "   'fcnet_activation': 'relu',\n",
       "   'conv_filters': None,\n",
       "   'conv_activation': 'relu',\n",
       "   'post_fcnet_hiddens': [],\n",
       "   'post_fcnet_activation': 'relu',\n",
       "   'free_log_std': False,\n",
       "   'no_final_linear': False,\n",
       "   'vf_share_layers': True,\n",
       "   'use_lstm': False,\n",
       "   'max_seq_len': 20,\n",
       "   'lstm_cell_size': 256,\n",
       "   'lstm_use_prev_action': False,\n",
       "   'lstm_use_prev_reward': False,\n",
       "   '_time_major': False,\n",
       "   'use_attention': False,\n",
       "   'attention_num_transformer_units': 1,\n",
       "   'attention_dim': 64,\n",
       "   'attention_num_heads': 1,\n",
       "   'attention_head_dim': 32,\n",
       "   'attention_memory_inference': 50,\n",
       "   'attention_memory_training': 50,\n",
       "   'attention_position_wise_mlp_dim': 32,\n",
       "   'attention_init_gru_gate_bias': 2.0,\n",
       "   'attention_use_n_prev_actions': 0,\n",
       "   'attention_use_n_prev_rewards': 0,\n",
       "   'framestack': True,\n",
       "   'dim': 84,\n",
       "   'grayscale': False,\n",
       "   'zero_mean': True,\n",
       "   'custom_model': None,\n",
       "   'custom_model_config': {},\n",
       "   'custom_action_dist': None,\n",
       "   'custom_preprocessor': None,\n",
       "   'lstm_use_prev_action_reward': -1,\n",
       "   '_use_default_native_models': -1},\n",
       "  'optimizer': {},\n",
       "  'max_requests_in_flight_per_sampler_worker': 2,\n",
       "  'rl_trainer_class': None,\n",
       "  '_enable_rl_trainer_api': False,\n",
       "  '_rl_trainer_hps': RLTrainerHPs(),\n",
       "  'explore': True,\n",
       "  'exploration_config': {'type': 'EpsilonGreedy',\n",
       "   'initial_epsilon': 1.0,\n",
       "   'final_epsilon': 0.02,\n",
       "   'epsilon_timesteps': 10000},\n",
       "  'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec at 0x7fdc13ab7f40>},\n",
       "  'policy_states_are_swappable': False,\n",
       "  'input_config': {},\n",
       "  'actions_in_input_normalized': False,\n",
       "  'postprocess_inputs': False,\n",
       "  'shuffle_buffer_size': 0,\n",
       "  'output': None,\n",
       "  'output_config': {},\n",
       "  'output_compress_columns': ['obs', 'new_obs'],\n",
       "  'output_max_file_size': 67108864,\n",
       "  'offline_sampling': False,\n",
       "  'evaluation_interval': None,\n",
       "  'evaluation_duration': 10,\n",
       "  'evaluation_duration_unit': 'episodes',\n",
       "  'evaluation_sample_timeout_s': 180.0,\n",
       "  'evaluation_parallel_to_training': False,\n",
       "  'evaluation_config': {'explore': False},\n",
       "  'off_policy_estimation_methods': {},\n",
       "  'ope_split_batch_by_episode': True,\n",
       "  'evaluation_num_workers': 0,\n",
       "  'always_attach_evaluation_results': False,\n",
       "  'enable_async_evaluation': False,\n",
       "  'in_evaluation': False,\n",
       "  'sync_filters_on_rollout_workers_timeout_s': 60.0,\n",
       "  'keep_per_episode_custom_metrics': False,\n",
       "  'metrics_episode_collection_timeout_s': 60.0,\n",
       "  'metrics_num_episodes_for_smoothing': 100,\n",
       "  'min_time_s_per_iteration': None,\n",
       "  'min_train_timesteps_per_iteration': 0,\n",
       "  'min_sample_timesteps_per_iteration': 1000,\n",
       "  'export_native_model_files': False,\n",
       "  'checkpoint_trainable_policies_only': False,\n",
       "  'logger_creator': None,\n",
       "  'logger_config': None,\n",
       "  'log_level': 'WARN',\n",
       "  'log_sys_usage': True,\n",
       "  'fake_sampler': False,\n",
       "  'seed': None,\n",
       "  'worker_cls': None,\n",
       "  'rl_module_class': None,\n",
       "  '_enable_rl_module_api': False,\n",
       "  '_tf_policy_handles_more_than_one_loss': False,\n",
       "  '_disable_preprocessor_api': False,\n",
       "  '_disable_action_flattening': False,\n",
       "  '_disable_execution_plan_api': True,\n",
       "  'simple_optimizer': False,\n",
       "  'replay_sequence_length': None,\n",
       "  'horizon': -1,\n",
       "  'soft_horizon': -1,\n",
       "  'no_done_at_end': -1,\n",
       "  'target_network_update_freq': 500,\n",
       "  'replay_buffer_config': {'type': ray.rllib.utils.replay_buffers.multi_agent_prioritized_replay_buffer.MultiAgentPrioritizedReplayBuffer,\n",
       "   'prioritized_replay': -1,\n",
       "   'capacity': 50000,\n",
       "   'prioritized_replay_alpha': 0.6,\n",
       "   'prioritized_replay_beta': 0.4,\n",
       "   'prioritized_replay_eps': 1e-06,\n",
       "   'replay_sequence_length': 1,\n",
       "   'worker_side_prioritization': False},\n",
       "  'num_steps_sampled_before_learning_starts': 1000,\n",
       "  'store_buffer_in_checkpoints': False,\n",
       "  'lr_schedule': None,\n",
       "  'adam_epsilon': 1e-08,\n",
       "  'grad_clip': 40,\n",
       "  'tau': 1.0,\n",
       "  'num_atoms': 1,\n",
       "  'v_min': -10.0,\n",
       "  'v_max': 10.0,\n",
       "  'noisy': False,\n",
       "  'sigma0': 0.5,\n",
       "  'dueling': True,\n",
       "  'hiddens': [256],\n",
       "  'double_q': True,\n",
       "  'n_step': 1,\n",
       "  'before_learn_on_batch': None,\n",
       "  'training_intensity': None,\n",
       "  'td_error_loss_fn': 'huber',\n",
       "  'categorical_distribution_temperature': 1.0,\n",
       "  'input': 'sampler',\n",
       "  'multiagent': {'policies': {'default_policy': (None, None, None, None)},\n",
       "   'policy_mapping_fn': <function ray.rllib.algorithms.algorithm_config.AlgorithmConfig.__init__.<locals>.<lambda>(aid, episode, worker, **kwargs)>,\n",
       "   'policies_to_train': None,\n",
       "   'policy_map_capacity': 100,\n",
       "   'policy_map_cache': -1,\n",
       "   'count_steps_by': 'env_steps',\n",
       "   'observation_fn': None},\n",
       "  'callbacks': ray.rllib.algorithms.callbacks.DefaultCallbacks,\n",
       "  'create_env_on_driver': False,\n",
       "  'custom_eval_function': None,\n",
       "  'framework': 'tf',\n",
       "  'num_cpus_for_driver': 1,\n",
       "  'num_workers': 1},\n",
       " 'time_since_restore': 112.43479704856873,\n",
       " 'timesteps_since_restore': 0,\n",
       " 'iterations_since_restore': 4,\n",
       " 'warmup_time': 19.03680729866028,\n",
       " 'perf': {'cpu_util_percent': 16.025, 'ram_util_percent': 69.9576923076923}}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'EpsilonGreedy', 'initial_epsilon': 1.0, 'final_epsilon': 0.02, 'epsilon_timesteps': 10000}\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.dqn.dqn import DQNConfig\n",
    "config = DQNConfig()\n",
    "print(config.exploration_config)  \n",
    "explore_config = config.exploration_config.update( \n",
    "    {\n",
    "        \"initial_epsilon\": 1.5,\n",
    "        \"final_epsilon\": 0.01,\n",
    "        \"epsilone_timesteps\": 5000,\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "22a2d9d2260537ac5d5344e0960edf424aea8acd2950750b938407e188415f8b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
