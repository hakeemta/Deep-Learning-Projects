{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 7\n",
    "# Tensorflow Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from copy import deepcopy\n",
    "import time\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "# what ever framework you fancy\n",
    "from collections import namedtuple, deque \n",
    "import tensorflow as tf\n",
    "from  tensorflow.keras.layers import Dense\n",
    "from  tensorflow.keras import Sequential,Input\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up environment\n",
    "env_kwargs = {\n",
    "    \"id\": \"LunarLander-v2\",\n",
    "    \"continuous\": False,\n",
    "    \"gravity\" : -8.0,\n",
    "    \"enable_wind\": False,\n",
    "}\n",
    "env = gym.make(**env_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space: Box([-1.5       -1.5       -5.        -5.        -3.1415927 -5.\n",
      " -0.        -0.       ], [1.5       1.5       5.        5.        3.1415927 5.        1.\n",
      " 1.       ], (8,), float32), \n",
      "high: [1.5       1.5       5.        5.        3.1415927 5.        1.\n",
      " 1.       ], \n",
      "low: [-1.5       -1.5       -5.        -5.        -3.1415927 -5.\n",
      " -0.        -0.       ]\n",
      "action space: Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "# get some information about the env dimensions\n",
    "print(f'observation space: {env.observation_space}, \\nhigh: {env.observation_space.high}, \\nlow: {env.observation_space.low}')\n",
    "print(f'action space: {env.action_space}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement replay buffer\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Memory buffer for experience replay of transitions in episodes\n",
    "\n",
    "    Args:\n",
    "        capacity (int): max capacity of the buffer\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = deque([], maxlen=self.capacity) \n",
    "\n",
    "\n",
    "    def push(self, transition):\n",
    "        \"\"\"Save transition to buffer.\"\"\"\n",
    "        return self.buffer.append(transition)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Randomly sample a batch of transitions of specified size from the buffer\"\"\"        \n",
    "        return random.choices(self.buffer, k=batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Gives the length of the current buffer\"\"\"\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement deep Q-network\n",
    "\n",
    "class DQN:\n",
    "    \"\"\"Deep FF Q-Network for Gym classical envs\n",
    "    consisting of 3 linear layers with ReLU activation\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): dimension of input, shape of observation space\n",
    "        output_dim (int): dimension of output, number of possible actions\n",
    "        hidden_dim (int): number of units in hidden layer\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=128, loss=\"mse\", optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "    def create_model(self):\n",
    "        self.model = Sequential([\n",
    "            Input(shape=(self.input_dim)),\n",
    "            Dense(self.hidden_dim, activation=\"relu\"),\n",
    "            Dense(self.output_dim )]\n",
    "        )\n",
    "        self.model.compile(loss=self.loss, optimizer=self.optimizer)\n",
    "        return self.model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Trainer:\n",
    "    \"\"\"Trainer class for training of DQN on classical Gym environments\n",
    "\n",
    "    Args:\n",
    "        env (gym.Env):  environment to train\n",
    "        model (nn.Module):  deep Q-Network model\n",
    "        batch_size (int):   size of mini batch\n",
    "        gamma (float):  discount factor gamma for MDP\n",
    "        use_target (bool):  use target network flag (double DQN)\n",
    "        target_update_iters (int):  update target NN every target_update_iters iterations\n",
    "        capacity (int): capacity of replay buffer\n",
    "        epsilon_start (float):  starting value of epsilon\n",
    "        epsilon_end (float):    final value of epsilon\n",
    "        epsilon_decay_iters (int):  number of iters to decay from start to final value\n",
    "        lr (float): learning rate for optimizer\n",
    "        clip_grad (int or float):   value for gradient clipping, no clipping if 0\n",
    "        report_iters (int): report mean results every report_iters iterations\n",
    "        seed (int): seed for RNG\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 env_kwargs,\n",
    "                 model, \n",
    "                 batch_size=64, \n",
    "                 gamma=0.99,\n",
    "                 use_target=True, \n",
    "                 target_update_iters=20, \n",
    "                 capacity=500,\n",
    "                 epsilon_start=1.0, \n",
    "                 epsilon_end=0.01, \n",
    "                 epsilon_decay_iters=4000,\n",
    "                 lr=0.005, \n",
    "                 clip_grad=1.0, \n",
    "                 report_iters=50,\n",
    "                 seed: int = 1):\n",
    "        # env params\n",
    "        self.env_kwargs = env_kwargs\n",
    "        self.env = gym.make(**self.env_kwargs)\n",
    "        self.n_actions = self.env.action_space.n\n",
    "\n",
    "        # model params\n",
    "        self.model = model\n",
    "        self.use_target = use_target\n",
    "        if self.use_target:\n",
    "            self.target = deepcopy(self.model)\n",
    "            self.target_update_iters = target_update_iters\n",
    "            \n",
    "        self.replay = ReplayBuffer(capacity=capacity)\n",
    "\n",
    "        # meta params\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay_iters\n",
    "        self.clip_grad = clip_grad\n",
    "        self.report_iters = report_iters\n",
    "        self.seed = seed\n",
    "        self._rnds = np.random.RandomState(seed)\n",
    "        self._step = 0\n",
    "\n",
    "    def train(self, num_episodes):\n",
    "        \"\"\"Training of network model\"\"\"\n",
    "        print(f'Start training model for {num_episodes}:')\n",
    "        reward_report = deque(maxlen=self.report_iters)\n",
    "        loss_report = deque(maxlen=self.report_iters)\n",
    "        for i in range(num_episodes):\n",
    "\n",
    "            reward_report.append(self.do_episode())\n",
    "            loss_report.append(self.update_model())\n",
    "\n",
    "            # update target network\n",
    "            if self.use_target and i % self.target_update_iters == 0:\n",
    "                self.update_target()\n",
    "\n",
    "            # report\n",
    "            if i % self.report_iters == 0:\n",
    "                mean_r = np.mean(reward_report)\n",
    "                try:\n",
    "                    mean_l = np.mean(loss_report)\n",
    "                except TypeError:\n",
    "                    mean_l = 0\n",
    "                print(f'eps: {i:04} - mean reward/loss over last {self.report_iters} episodes: '\n",
    "                      f'{mean_r:.1f}/{mean_l:.4f}')\n",
    "\n",
    "        print('training finished.')\n",
    "\n",
    "    def _decay_eps_threshold(self):\n",
    "        \"\"\"Step decay of threshold for epsilon greedy\"\"\"\n",
    "        return self.epsilon_end + \\\n",
    "               (self.epsilon_start - self.epsilon_end) * \\\n",
    "               np.exp(-1. * self._step / self.epsilon_decay)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"Do forward pass and get action from model\"\"\"\n",
    "        t = self._decay_eps_threshold()     # update threshold\n",
    "        r = self._rnds.rand()    # sample from [0, 1]\n",
    "        self._step += 1\n",
    "        if r < t:   # do random action\n",
    "            \n",
    "            return tf.math.argmax(self.model(state),-1).numpy()[0]\n",
    "        \n",
    "        else:   # greedy action\n",
    "            return  self.env.action_space.sample()\n",
    "\n",
    "\n",
    "    def do_episode(self, render=False):\n",
    "        \"\"\"Do one episode in environment\"\"\"\n",
    "        if render:\n",
    "            self.env = gym.make(**self.env_kwargs, render_mode=\"human\")\n",
    "        stop = False\n",
    "        total_reward = 0\n",
    "        state, _ = self.env.reset(seed=self.seed)\n",
    "        state = tf.convert_to_tensor(state.reshape(-1,8), dtype=tf.float32)\n",
    "\n",
    "        while not stop:\n",
    "            action = self.get_action(state)            \n",
    "            next_state, reward, term, trunc, info = self.env.step(action)\n",
    "            stop = term or trunc\n",
    "            \n",
    "            # amend reward --> reward shaping \n",
    "            if next_state[0] >= 0.1:\n",
    "                reward += 10\n",
    "            elif next_state[0] >= 0.25:\n",
    "                reward += 20\n",
    "            elif next_state[0] >= 0.5:\n",
    "                reward += 100\n",
    "\n",
    "            # render env\n",
    "            if render:\n",
    "                self.env.render()\n",
    "                time.sleep(0.05)\n",
    "\n",
    "            total_reward += reward\n",
    "\n",
    "            next_state = tf.convert_to_tensor(next_state.reshape(-1,8), dtype=tf.float32)\n",
    "            reward = tf.convert_to_tensor(reward,dtype=tf.float32)\n",
    "\n",
    "            # save transition\n",
    "            self.replay.push((tf.squeeze(state), action, tf.squeeze(next_state), reward,stop))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        return total_reward\n",
    "\n",
    "\n",
    "    def update_model(self):\n",
    "        \"\"\"Do batch update of model parameters\"\"\"\n",
    "        if len(self.replay) <= self.batch_size:\n",
    "            return\n",
    "\n",
    "        # get transitions from buffer\n",
    "        train_batch = self.replay.sample(self.batch_size)\n",
    "    \n",
    "\n",
    "        input = []\n",
    "        output = []\n",
    "        # calculate Q(s_t, a)\n",
    "        current_states = np.array([ transition[0] for transition in train_batch ])\n",
    "        current_q_list = self.model.predict(current_states)\n",
    "        \n",
    "        # calculate V(s_{t+1}) either with target network or original model\n",
    "        future_states = np.array([ transition[2] for transition in train_batch ])\n",
    "        future_q_list = self.target(future_states)\n",
    "        print(future_q_list.shape)\n",
    "        \n",
    "        # mask final states\n",
    "        # calculate expected Q-values\n",
    "        for idx, (state, action, next_state, reward,stop) in enumerate(train_batch):\n",
    "            if not stop:\n",
    "                max_next_q = tf.math.reduce_max(future_q_list[idx])\n",
    "                print('max_next_q:',max_next_q)\n",
    "                new_q = reward + self.gamma * max_next_q\n",
    "                print('new_q:',new_q)\n",
    "            else:\n",
    "                new_q = reward\n",
    "        \n",
    "            current_q = current_q_list[idx]\n",
    "            print('current_q:',current_q)\n",
    "            current_q[action] = new_q\n",
    "            print(f'current_q: of action {action} is', current_q[action])\n",
    "            \n",
    "            input.append(state)\n",
    "            output.append(current_q)\n",
    "            break\n",
    "        \n",
    "        print(len(input), len(output))\n",
    "        # update model params\n",
    "        history = self.model.fit(input,output, batch_size=self.batch_size, verbose=0, shuffle=False)\n",
    "        loss = history.history['loss']\n",
    "        \n",
    "\n",
    "        return loss\n",
    "\n",
    "    def update_target(self):\n",
    "        \"\"\"Update target network with parameters of model\"\"\"\n",
    "        return self.target.set_weights(deepcopy(self.model.get_weights()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 123\n",
    "N = 3000\n",
    "env_kwargs = {\n",
    "    \"id\": \"LunarLander-v2\",\n",
    "    \"continuous\": False,\n",
    "    \"gravity\" : -8.0,\n",
    "    \"enable_wind\": False,\n",
    "}\n",
    "\n",
    "\n",
    "# initialize model\n",
    "model = DQN(8, 4,).create_model()\n",
    "\n",
    "# initialize trainer\n",
    "trainer = Trainer(env_kwargs, model, batch_size=256, seed=SEED, epsilon_decay_iters=N*0.95, lr=0.001, clip_grad=0.1)\n",
    "\n",
    "# train...\n",
    "trainer.train(N)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Oct 24 2022, 11:04:34) [Clang 12.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "22a2d9d2260537ac5d5344e0960edf424aea8acd2950750b938407e188415f8b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
