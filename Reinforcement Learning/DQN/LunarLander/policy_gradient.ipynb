{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from collections import defaultdict \n",
    "import tensorflow as tf\n",
    "from  tensorflow.keras.layers import Dense\n",
    "from  tensorflow.keras import Sequential,Input\n",
    "import tensorflow_probability as tfp\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED=3108\n",
    "tf.keras.utils.set_random_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up environment\n",
    "env_kwargs = {\n",
    "    \"id\": \"LunarLander-v2\",\n",
    "    \"continuous\": False,\n",
    "    \"gravity\" : -8.0,\n",
    "    \"enable_wind\": False,\n",
    "}\n",
    "env = gym.make(**env_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space: Box([-1.5       -1.5       -5.        -5.        -3.1415927 -5.\n",
      " -0.        -0.       ], [1.5       1.5       5.        5.        3.1415927 5.        1.\n",
      " 1.       ], (8,), float32), high: [1.5       1.5       5.        5.        3.1415927 5.        1.\n",
      " 1.       ], low: [-1.5       -1.5       -5.        -5.        -3.1415927 -5.\n",
      " -0.        -0.       ]\n",
      "action space: Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "# get some information about the env dimensions\n",
    "print(f'observation space: {env.observation_space}, high: {env.observation_space.high}, low: {env.observation_space.low}')\n",
    "print(f'action space: {env.action_space}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork():\n",
    "    \"\"\"Deep FFNetwork for Gym classical envs\n",
    "    consisting of 3 linear layers with ReLU activation\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): dimension of input, shape of observation space\n",
    "        output_dim (int): dimension of output, number of possible actions\n",
    "        hidden_dim (int): number of units in hidden layer\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=128):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "    def create_model(self):\n",
    "        self.model = Sequential([\n",
    "            Input(shape=(self.input_dim,)),\n",
    "            Dense(self.hidden_dim, activation=\"sigmoid\"),\n",
    "            Dense(self.output_dim )]\n",
    "        )\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \"\"\"Trainer class for REINFORCE training of Policy NN on classical Gym environments\n",
    "\n",
    "    Args:\n",
    "        env_kwargs:  params for env\n",
    "        model (nn.Module):  deep Q-Network model\n",
    "        gamma (float):  discount factor gamma for MDP\n",
    "        lr (float): learning rate for optimizer\n",
    "        entropy_coeff (float): coefficient for entropy regularization\n",
    "        clip_grad (int or float):   value for gradient clipping, no clipping if 0\n",
    "        report_iters (int): report mean results every report_iters iterations\n",
    "        seed (int): seed for RNG\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 env_kwargs,\n",
    "                 model,\n",
    "                 gamma=0.99,\n",
    "                 lr=0.001,\n",
    "                 entropy_coeff=0.0,\n",
    "                 clip_grad=1.0,\n",
    "                 report_iters=50,\n",
    "                 seed: int = 1\n",
    "                 ):\n",
    "        # env params\n",
    "        self.env_kwargs = env_kwargs\n",
    "        self.env = gym.make(**self.env_kwargs)\n",
    "        self.n_actions = self.env.action_space.n\n",
    "\n",
    "        # model params\n",
    "        self.model = model\n",
    "\n",
    "        self.lr = lr\n",
    "        self.entropy_coeff = entropy_coeff\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.lr)\n",
    "\n",
    "        # meta params\n",
    "        self.gamma = gamma\n",
    "        self.clip_grad = clip_grad\n",
    "        self.report_iters = report_iters\n",
    "\n",
    "        self.rollout = None\n",
    "        self.seed = seed\n",
    "        self._rnds = np.random.RandomState(seed)\n",
    "        self._step = 0\n",
    "        \n",
    "        \n",
    "        \n",
    "    def train(self, num_episodes):\n",
    "        \"\"\"Training of network model\"\"\"\n",
    "        print(f'Start training model for {num_episodes} episodes:')\n",
    "        reward_report = deque(maxlen=self.report_iters)\n",
    "        loss_report = deque(maxlen=self.report_iters)\n",
    "        for i in range(num_episodes):\n",
    "\n",
    "            reward_report.append(self.do_episode())\n",
    "            tot_loss, loss, entropy= self.update_model()\n",
    "            loss_report.append(tot_loss)\n",
    "\n",
    "            # report\n",
    "            if i % self.report_iters == 0:\n",
    "                mean_r = np.mean(reward_report)\n",
    "                try:\n",
    "                    mean_l = np.mean(loss_report)\n",
    "                except TypeError:\n",
    "                    mean_l = 0\n",
    "                print(f'eps: {i:04} - mean reward/loss over last {self.report_iters} episodes: '\n",
    "                      f'{mean_r:.1f}/{mean_l:.4f}, entropy: {entropy:.4f}')\n",
    "\n",
    "        print('training finished.')\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        \"\"\"Do forward pass and sample an action from model\"\"\"\n",
    "        logprob = self.model.predict(state)\n",
    "        \n",
    "        # We need to sample an action from logprob\n",
    "        # Doing this gives the balance between exploitation and exploration\n",
    "        distribution = tfp.distributions.Categorical(logits=logprob)\n",
    "        action = distribution.sample().numpy().item()\n",
    "        entropy = distribution.entropy().numpy().item()\n",
    "        return action, logprob, entropy        \n",
    "        \n",
    "       \n",
    "    def do_episode(self, render=False):\n",
    "        \"\"\"Do one episode in environment\"\"\"\n",
    "        if render:\n",
    "            self.env = gym.make(**self.env_kwargs, render_mode=\"human\")\n",
    "        stop = False\n",
    "        total_reward = 0\n",
    "        state, _ = self.env.reset(seed=self.seed)\n",
    "        state = tf.convert_to_tensor(state.reshape(-1,8), dtype=tf.float32)\n",
    "\n",
    "        self.rollout = defaultdict(list)\n",
    "\n",
    "        while not stop:\n",
    "            self.rollout['state'].append(state)\n",
    "            action, logprob, entropy = self.get_action(state)\n",
    "            next_state, reward, term, trunc, info = self.env.step(action)\n",
    "            stop = term or trunc\n",
    "\n",
    "            # render env\n",
    "            if render:\n",
    "                self.env.render()\n",
    "                time.sleep(0.05)\n",
    "\n",
    "            total_reward += reward\n",
    "\n",
    "            next_state = tf.convert_to_tensor(next_state.reshape(-1,8), dtype=tf.float32)\n",
    "            reward = reward\n",
    "\n",
    "            # save transition to rollout (reward, logprob, entropy, ...)\n",
    "            self.rollout['reward'].append(reward)\n",
    "            self.rollout['logprob'].append(logprob)\n",
    "            self.rollout['entropy'].append(entropy)\n",
    "            state = next_state\n",
    "\n",
    "        return total_reward\n",
    "    \n",
    "    \n",
    "\n",
    "    def update_model(self):\n",
    "        \"\"\"Do batch update of model parameters\"\"\"\n",
    "\n",
    "        # get transitions from rollout\n",
    "        rewards = self.rollout['reward']\n",
    "        logprob = self.rollout['logprob']\n",
    "        entropy = self.rollout['entropy']\n",
    "        state = tf.concat(self.rollout['state'], axis=0)\n",
    "        \n",
    "\n",
    "        # calculate discounted cumulative reward\n",
    "        discounted_rewards = np.power(self.gamma, np.arange(len(rewards))) * rewards\n",
    "        cumulative_rewards = np.cumsum(discounted_rewards[::-1])[::-1]\n",
    "        \n",
    "        # score rewards by logprobs\n",
    "        with tf.GradientTape() as tape:\n",
    "            # We need to maximize\n",
    "            # Recompute log probability, initial logprob is disconnected\n",
    "            score = - self.model(state) * tf.concat(cumulative_rewards, axis=0)#np.mean(cumulative_rewards)  \n",
    "        gradients = tape.gradient(score, self.model.trainable_weights)\n",
    "        # update params\n",
    "        #ToDo: maximize not minimize\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_weights))\n",
    "        total_loss = None\n",
    "        loss= None  \n",
    "         \n",
    "\n",
    "        return total_loss, loss, np.array(entropy).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training model for 1500 episodes:\n",
      "1/1 [==============================] - 0s 154ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "cannot compute Mul as input #1(zero-based) was expected to be a float tensor but is a double tensor [Op:Mul]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m/Users/sophialawal/Visual_Studio/Deep_learning_projects/Reinforcement Learning/DQN/LunarLander/policy_gradient.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sophialawal/Visual_Studio/Deep_learning_projects/Reinforcement%20Learning/DQN/LunarLander/policy_gradient.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sophialawal/Visual_Studio/Deep_learning_projects/Reinforcement%20Learning/DQN/LunarLander/policy_gradient.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     env_kwargs,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sophialawal/Visual_Studio/Deep_learning_projects/Reinforcement%20Learning/DQN/LunarLander/policy_gradient.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sophialawal/Visual_Studio/Deep_learning_projects/Reinforcement%20Learning/DQN/LunarLander/policy_gradient.ipynb#W5sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     clip_grad\u001b[39m=\u001b[39m\u001b[39m0.08\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sophialawal/Visual_Studio/Deep_learning_projects/Reinforcement%20Learning/DQN/LunarLander/policy_gradient.ipynb#W5sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sophialawal/Visual_Studio/Deep_learning_projects/Reinforcement%20Learning/DQN/LunarLander/policy_gradient.ipynb#W5sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# train...\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sophialawal/Visual_Studio/Deep_learning_projects/Reinforcement%20Learning/DQN/LunarLander/policy_gradient.ipynb#W5sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain(N)\n",
      "\u001b[1;32m/Users/sophialawal/Visual_Studio/Deep_learning_projects/Reinforcement Learning/DQN/LunarLander/policy_gradient.ipynb Cell 6\u001b[0m in \u001b[0;36mTrainer.train\u001b[0;34m(self, num_episodes)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sophialawal/Visual_Studio/Deep_learning_projects/Reinforcement%20Learning/DQN/LunarLander/policy_gradient.ipynb#W5sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_episodes):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sophialawal/Visual_Studio/Deep_learning_projects/Reinforcement%20Learning/DQN/LunarLander/policy_gradient.ipynb#W5sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     reward_report\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_episode(render\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sophialawal/Visual_Studio/Deep_learning_projects/Reinforcement%20Learning/DQN/LunarLander/policy_gradient.ipynb#W5sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m     tot_loss, loss, entropy\u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate_model()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sophialawal/Visual_Studio/Deep_learning_projects/Reinforcement%20Learning/DQN/LunarLander/policy_gradient.ipynb#W5sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     loss_report\u001b[39m.\u001b[39mappend(tot_loss)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sophialawal/Visual_Studio/Deep_learning_projects/Reinforcement%20Learning/DQN/LunarLander/policy_gradient.ipynb#W5sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     \u001b[39m# report\u001b[39;00m\n",
      "\u001b[1;32m/Users/sophialawal/Visual_Studio/Deep_learning_projects/Reinforcement Learning/DQN/LunarLander/policy_gradient.ipynb Cell 6\u001b[0m in \u001b[0;36mTrainer.update_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sophialawal/Visual_Studio/Deep_learning_projects/Reinforcement%20Learning/DQN/LunarLander/policy_gradient.ipynb#W5sZmlsZQ%3D%3D?line=133'>134</a>\u001b[0m \u001b[39m# score rewards by logprobs\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sophialawal/Visual_Studio/Deep_learning_projects/Reinforcement%20Learning/DQN/LunarLander/policy_gradient.ipynb#W5sZmlsZQ%3D%3D?line=134'>135</a>\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m tape:\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sophialawal/Visual_Studio/Deep_learning_projects/Reinforcement%20Learning/DQN/LunarLander/policy_gradient.ipynb#W5sZmlsZQ%3D%3D?line=135'>136</a>\u001b[0m     \u001b[39m# We need to maximize\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sophialawal/Visual_Studio/Deep_learning_projects/Reinforcement%20Learning/DQN/LunarLander/policy_gradient.ipynb#W5sZmlsZQ%3D%3D?line=136'>137</a>\u001b[0m     \u001b[39m# Recompute log probability, initial logprob is disconnected\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/sophialawal/Visual_Studio/Deep_learning_projects/Reinforcement%20Learning/DQN/LunarLander/policy_gradient.ipynb#W5sZmlsZQ%3D%3D?line=137'>138</a>\u001b[0m     score \u001b[39m=\u001b[39m \u001b[39m-\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(state) \u001b[39m*\u001b[39;49m tf\u001b[39m.\u001b[39;49mconcat(cumulative_rewards, axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\u001b[39m#np.mean(cumulative_rewards)  \u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sophialawal/Visual_Studio/Deep_learning_projects/Reinforcement%20Learning/DQN/LunarLander/policy_gradient.ipynb#W5sZmlsZQ%3D%3D?line=138'>139</a>\u001b[0m gradients \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39mgradient(score, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrainable_weights)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sophialawal/Visual_Studio/Deep_learning_projects/Reinforcement%20Learning/DQN/LunarLander/policy_gradient.ipynb#W5sZmlsZQ%3D%3D?line=139'>140</a>\u001b[0m \u001b[39m# update params\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sophialawal/Visual_Studio/Deep_learning_projects/Reinforcement%20Learning/DQN/LunarLander/policy_gradient.ipynb#W5sZmlsZQ%3D%3D?line=140'>141</a>\u001b[0m \u001b[39m#ToDo: maximize not minimize\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7215\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   7214\u001b[0m   e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 7215\u001b[0m   \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: cannot compute Mul as input #1(zero-based) was expected to be a float tensor but is a double tensor [Op:Mul]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "N = 1500\n",
    "\n",
    "# initialize model\n",
    "model = PolicyNetwork(8,4,128).create_model()\n",
    "\n",
    "# initialize trainer\n",
    "trainer = Trainer(\n",
    "    env_kwargs,\n",
    "    model,\n",
    "    seed=SEED,\n",
    "    lr=0.0001,\n",
    "    clip_grad=0.08\n",
    ")\n",
    "\n",
    "# train...\n",
    "trainer.train(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step\n",
      "-0.8480336584644306\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "-0.9689037396067306\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "-0.9690818968023791\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "-0.9674164884776815\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "-0.963405975250339\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "-0.9570794142265697\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "-0.9484766066740065\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "-0.9377631421919546\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "-0.9250610403839801\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "-0.9104710706787387\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "-0.8941562395535243\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "-0.8762142538319893\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "-0.8567419612757305\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "-0.8358999594615568\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "-0.8137722013105986\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "-0.790458801527052\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "-0.7660229514430625\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "-0.740673974222716\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "-0.71437350526773\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "-0.6872765209890588\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "-0.6594836856118889\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "-0.631065088244668\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "-0.6020874508883196\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "-0.5726617210344216\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "-0.6104168734147208\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "-0.5103605658588037\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "-0.48013676560245244\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "-0.44982304279614027\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "-0.41944935721903676\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "-0.38920622851441067\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "-0.35911648930456863\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "-0.32932675946287304\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "-0.299989090678622\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "-0.27123445629993626\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "-0.24317616610056803\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "-0.2160419745934803\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "-0.19000736958440712\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "-0.16531723047705782\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "-0.1422364112531227\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "-0.12108484372873818\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "-0.10226846288318825\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "-0.08619888917263552\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "-0.07338688648525249\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "-0.06454717631595486\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "-0.06039984194336512\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "-0.061916929857460445\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "-0.07023547589247414\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "-0.08678827484317253\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "-0.11331085934756402\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "-0.15191208346271878\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "-0.20518847194887258\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "-0.27634494122847286\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "-0.369104019330706\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "-0.4879231656033767\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "-0.6377588295126486\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "-0.8238483754817025\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "-1.0512143648330152\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "-1.3236343827266523\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "8.357551042193393\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "21.17330408072351\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "-100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-101.5486312802604"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.do_episode(render=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Oct 24 2022, 11:04:34) [Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "22a2d9d2260537ac5d5344e0960edf424aea8acd2950750b938407e188415f8b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
