{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "from collections import namedtuple, deque \n",
    "import tensorflow as tf\n",
    "from  tensorflow.keras.layers import Dense\n",
    "from  tensorflow.keras import Sequential,Input\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up environment\n",
    "env_kwargs = {\n",
    "    \"id\": \"LunarLander-v2\",\n",
    "    \"continuous\": False,\n",
    "    \"gravity\" : -8.0,\n",
    "    \"enable_wind\": False,\n",
    "}\n",
    "env = gym.make(**env_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some information about the env dimensions\n",
    "print(f'observation space: {env.observation_space}, high: {env.observation_space.high}, low: {env.observation_space.low}')\n",
    "print(f'action space: {env.action_space}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork:\n",
    "    \"\"\"Deep FFNetwork for Gym classical envs\n",
    "    consisting of 3 linear layers with ReLU activation\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): dimension of input, shape of observation space\n",
    "        output_dim (int): dimension of output, number of possible actions\n",
    "        hidden_dim (int): number of units in hidden layer\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=128):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "    def create_model(self):\n",
    "        self.model = Sequential([\n",
    "            Input(shape=(self.input_dim,)),\n",
    "            Dense(self.hidden_dim, activation=\"relu\"),\n",
    "            Dense(self.output_dim )]\n",
    "        )\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 123\n",
    "N = 3000\n",
    "\n",
    "# initialize model\n",
    "model = PolicyNetwork(8,4,128)\n",
    "\n",
    "# initialize trainer\n",
    "trainer = Trainer(\n",
    "    env_kwargs,\n",
    "    model,\n",
    "    seed=SEED,\n",
    "    lr=0.0001,\n",
    "    clip_grad=0.08\n",
    ")\n",
    "\n",
    "# train...\n",
    "trainer.train(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.do_episode(render=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6 (main, Oct 24 2022, 11:04:34) [Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "22a2d9d2260537ac5d5344e0960edf424aea8acd2950750b938407e188415f8b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
